{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a068cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8dcf4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file containing responses\n",
    "df = pd.read_csv('Dataset/encoded_response.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5805e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Target variable: Learning Objects Preference\n",
    "target = df[[\n",
    "    'Learning Objects [Slide presentation]',\n",
    "    'Learning Objects [Book]',\n",
    "    'Learning Objects [Lecture Note]',\n",
    "    'Learning Objects [Educational game]',\n",
    "    'Learning Objects [Video]',\n",
    "    'Learning Objects [Audio-recorded lecture]',\n",
    "    'Learning Objects [Animated instruction]',\n",
    "    'Learning Objects [Real object model]',\n",
    "    'Learning Objects [Mind Map]',\n",
    "    'Learning Objects [Multimedia content]',\n",
    "    'Learning Objects [Interactive Tool]',\n",
    "    'Learning Objects [Technology-supported learning include computer-based training systems]',\n",
    "    'Learning Objects [Intelligent computer-aided instruction systems]'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f57a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(target.columns, axis=1), target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd12559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a RandomForestClassifier for each learning object\n",
    "# classifiers = {}\n",
    "# for col in target.columns:\n",
    "#     classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     classifier.fit(X_train, y_train[col])\n",
    "#     classifiers[col] = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eea515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c48c59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty dictionary to hold the classifiers\n",
    "# best_estimators = {}\n",
    "\n",
    "# # Loop through each learning object\n",
    "# for col in target.columns:\n",
    "#     # Define the parameter grid for hyperparameter tuning\n",
    "#     param_grid = {\n",
    "#         'n_estimators': [100, 150, 200],  # Vary the number of trees\n",
    "#         'max_depth': [None, 10, 20, 30],  # Vary the maximum depth of trees\n",
    "#         # Add other hyperparameters to tune\n",
    "#     }\n",
    "    \n",
    "#     # Instantiate GridSearchCV for RandomForestClassifier\n",
    "#     grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "    \n",
    "#     # Fit the grid search to your data for the current learning object\n",
    "#     grid_search.fit(X_train, y_train[col])\n",
    "    \n",
    "#     # Get the best parameters and best estimator for the current learning object\n",
    "#     best_params = grid_search.best_params_\n",
    "#     best_estimator = grid_search.best_estimator_\n",
    "    \n",
    "#     # Store the best estimator in the classifiers dictionary\n",
    "#     best_estimators[col] = best_estimator\n",
    "    \n",
    "# joblib.dump(best_estimators, \"Model/rf_model_2.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05cdeb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59483871 0.58967742 0.59483871 0.60258065 0.59096774 0.58580645\n",
      " 0.61032258 0.60258065 0.58580645 0.59870968 0.59483871 0.58967742\n",
      " 0.57806452 0.58967742 0.59483871 0.58967742 0.58967742 0.5883871\n",
      " 0.57806452 0.57677419 0.59225806 0.59096774 0.59096774 0.59225806\n",
      " 0.60258065 0.60645161 0.60645161 0.59096774 0.60258065 0.60645161\n",
      " 0.60645161 0.59096774 0.58451613 0.5883871  0.6        0.58967742\n",
      " 0.5716129  0.58967742 0.57677419 0.57548387 0.5716129  0.58967742\n",
      " 0.57677419 0.57548387 0.5716129  0.58967742 0.57677419 0.57548387\n",
      " 0.59354839 0.58322581 0.59225806 0.59741935 0.58451613 0.59096774\n",
      " 0.60516129 0.59354839 0.58967742 0.61419355 0.61548387 0.60129032\n",
      " 0.58580645 0.58193548 0.59483871 0.58193548 0.58709677 0.58580645\n",
      " 0.59096774 0.59225806 0.58580645 0.59612903 0.58967742 0.57935484\n",
      " 0.57677419 0.5883871  0.58580645 0.5883871  0.57677419 0.5883871\n",
      " 0.58580645 0.5883871  0.58064516 0.6        0.58709677 0.57935484\n",
      " 0.57806452 0.58709677 0.58709677 0.59870968 0.57806452 0.58709677\n",
      " 0.58709677 0.59870968 0.57806452 0.58709677 0.58709677 0.59870968\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56903226 0.55741935 0.5716129  0.58064516 0.60387097 0.60774194\n",
      " 0.60516129 0.59483871 0.59354839 0.60129032 0.6        0.5883871\n",
      " 0.58580645 0.59483871 0.60774194 0.60645161 0.60774194 0.60387097\n",
      " 0.60645161 0.60258065 0.59225806 0.5883871  0.59483871 0.59870968\n",
      " 0.58064516 0.58451613 0.6        0.58451613 0.58064516 0.58451613\n",
      " 0.6        0.58451613 0.5883871  0.58967742 0.59483871 0.58064516\n",
      " 0.5883871  0.59225806 0.58322581 0.57419355 0.5883871  0.59225806\n",
      " 0.58322581 0.57419355 0.5883871  0.59225806 0.58322581 0.57419355\n",
      " 0.59096774 0.59870968 0.61548387 0.60903226 0.56387097 0.58322581\n",
      " 0.57806452 0.5716129  0.58451613 0.59225806 0.59354839 0.58322581\n",
      " 0.5716129  0.59612903 0.6        0.60645161 0.58064516 0.57806452\n",
      " 0.58451613 0.58193548 0.60387097 0.59354839 0.58580645 0.57548387\n",
      " 0.58580645 0.57548387 0.57419355 0.57677419 0.58580645 0.57548387\n",
      " 0.57419355 0.57677419 0.61935484 0.61032258 0.5883871  0.58451613\n",
      " 0.57290323 0.57806452 0.59096774 0.59612903 0.57290323 0.57806452\n",
      " 0.59096774 0.59612903 0.57290323 0.57806452 0.59096774 0.59612903\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59483871 0.58967742 0.59483871 0.60129032 0.59096774 0.58580645\n",
      " 0.61032258 0.60258065 0.58580645 0.59870968 0.59483871 0.58967742\n",
      " 0.57806452 0.58967742 0.59483871 0.58967742 0.58967742 0.5883871\n",
      " 0.57806452 0.57677419 0.59225806 0.59096774 0.59096774 0.59225806\n",
      " 0.60258065 0.60645161 0.60645161 0.59096774 0.60258065 0.60645161\n",
      " 0.60645161 0.59096774 0.58451613 0.5883871  0.6        0.58967742\n",
      " 0.5716129  0.58967742 0.57677419 0.57548387 0.5716129  0.58967742\n",
      " 0.57677419 0.57548387 0.5716129  0.58967742 0.57677419 0.57548387\n",
      " 0.59483871 0.58967742 0.59612903 0.60129032 0.58451613 0.59096774\n",
      " 0.60516129 0.59354839 0.58967742 0.61419355 0.61548387 0.60129032\n",
      " 0.58580645 0.58193548 0.59483871 0.58193548 0.58709677 0.58580645\n",
      " 0.59096774 0.59225806 0.58580645 0.59612903 0.58967742 0.57935484\n",
      " 0.57677419 0.5883871  0.58580645 0.5883871  0.57677419 0.5883871\n",
      " 0.58580645 0.5883871  0.58064516 0.6        0.58709677 0.57935484\n",
      " 0.57806452 0.58709677 0.58709677 0.59870968 0.57806452 0.58709677\n",
      " 0.58709677 0.59870968 0.57806452 0.58709677 0.58709677 0.59870968\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59483871 0.58967742 0.59483871 0.60258065 0.59096774 0.58580645\n",
      " 0.61032258 0.60258065 0.58580645 0.59870968 0.59483871 0.58967742\n",
      " 0.57806452 0.58967742 0.59483871 0.58967742 0.58967742 0.5883871\n",
      " 0.57806452 0.57677419 0.59225806 0.59096774 0.59096774 0.59225806\n",
      " 0.60258065 0.60645161 0.60645161 0.59096774 0.60258065 0.60645161\n",
      " 0.60645161 0.59096774 0.58451613 0.5883871  0.6        0.58967742\n",
      " 0.5716129  0.58967742 0.57677419 0.57548387 0.5716129  0.58967742\n",
      " 0.57677419 0.57548387 0.5716129  0.58967742 0.57677419 0.57548387\n",
      " 0.59354839 0.58322581 0.59225806 0.59741935 0.58451613 0.59096774\n",
      " 0.60516129 0.59354839 0.58967742 0.61419355 0.61548387 0.60129032\n",
      " 0.58580645 0.58193548 0.59483871 0.58193548 0.58709677 0.58580645\n",
      " 0.59096774 0.59225806 0.58580645 0.59612903 0.58967742 0.57935484\n",
      " 0.57677419 0.5883871  0.58580645 0.5883871  0.57677419 0.5883871\n",
      " 0.58580645 0.5883871  0.58064516 0.6        0.58709677 0.57935484\n",
      " 0.57806452 0.58709677 0.58709677 0.59870968 0.57806452 0.58709677\n",
      " 0.58709677 0.59870968 0.57806452 0.58709677 0.58709677 0.59870968\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59483871 0.58967742 0.59483871 0.60258065 0.59096774 0.58580645\n",
      " 0.61032258 0.60258065 0.58580645 0.59870968 0.59483871 0.58967742\n",
      " 0.57806452 0.58967742 0.59483871 0.58967742 0.58967742 0.5883871\n",
      " 0.57806452 0.57677419 0.59225806 0.59096774 0.59096774 0.59225806\n",
      " 0.60258065 0.60645161 0.60645161 0.59096774 0.60258065 0.60645161\n",
      " 0.60645161 0.59096774 0.58451613 0.5883871  0.6        0.58967742\n",
      " 0.5716129  0.58967742 0.57677419 0.57548387 0.5716129  0.58967742\n",
      " 0.57677419 0.57548387 0.5716129  0.58967742 0.57677419 0.57548387\n",
      " 0.59354839 0.58322581 0.59225806 0.59741935 0.58451613 0.59096774\n",
      " 0.60516129 0.59354839 0.58967742 0.61419355 0.61548387 0.60129032\n",
      " 0.58580645 0.58193548 0.59483871 0.58193548 0.58709677 0.58580645\n",
      " 0.59096774 0.59225806 0.58580645 0.59612903 0.58967742 0.57935484\n",
      " 0.57677419 0.5883871  0.58580645 0.5883871  0.57677419 0.5883871\n",
      " 0.58580645 0.5883871  0.58064516 0.6        0.58709677 0.57935484\n",
      " 0.57806452 0.58709677 0.58709677 0.59870968 0.57806452 0.58709677\n",
      " 0.58709677 0.59870968 0.57806452 0.58709677 0.58709677 0.59870968]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Slide presentation]: 0.5868725868725869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.73290323 0.7316129  0.7316129  0.73290323 0.73290323 0.73419355\n",
      " 0.73548387 0.73419355 0.71741935 0.72516129 0.72516129 0.72645161\n",
      " 0.73419355 0.73677419 0.73548387 0.73419355 0.73548387 0.73548387\n",
      " 0.73419355 0.73677419 0.71870968 0.71870968 0.71741935 0.71741935\n",
      " 0.71354839 0.71741935 0.71612903 0.71483871 0.71354839 0.71741935\n",
      " 0.71612903 0.71483871 0.71354839 0.71483871 0.71612903 0.71483871\n",
      " 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645\n",
      " 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645\n",
      " 0.73806452 0.73548387 0.73548387 0.73419355 0.7316129  0.73548387\n",
      " 0.73290323 0.73290323 0.71225806 0.71483871 0.71483871 0.71354839\n",
      " 0.7316129  0.7316129  0.73419355 0.7316129  0.73032258 0.73032258\n",
      " 0.73032258 0.7316129  0.71483871 0.71612903 0.71225806 0.71096774\n",
      " 0.71225806 0.70967742 0.7083871  0.7083871  0.71225806 0.70967742\n",
      " 0.7083871  0.7083871  0.70967742 0.70967742 0.7083871  0.70967742\n",
      " 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613\n",
      " 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.73806452 0.73935484 0.73806452 0.73935484 0.73290323 0.73548387\n",
      " 0.73548387 0.73677419 0.72       0.72129032 0.72387097 0.72258065\n",
      " 0.73548387 0.73548387 0.73419355 0.73290323 0.73032258 0.73548387\n",
      " 0.73290323 0.73419355 0.72       0.71870968 0.71870968 0.71870968\n",
      " 0.71612903 0.71483871 0.71612903 0.71354839 0.71612903 0.71483871\n",
      " 0.71612903 0.71354839 0.71354839 0.71225806 0.71225806 0.71225806\n",
      " 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645\n",
      " 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645\n",
      " 0.73419355 0.73419355 0.73419355 0.73548387 0.73419355 0.73419355\n",
      " 0.73290323 0.73290323 0.71225806 0.71354839 0.71354839 0.71483871\n",
      " 0.73032258 0.72903226 0.72903226 0.7316129  0.72903226 0.73032258\n",
      " 0.7316129  0.7316129  0.71354839 0.71483871 0.71483871 0.71225806\n",
      " 0.71225806 0.71096774 0.70967742 0.7083871  0.71225806 0.71096774\n",
      " 0.70967742 0.7083871  0.71096774 0.7083871  0.7083871  0.7083871\n",
      " 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613\n",
      " 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.73290323 0.7316129  0.7316129  0.73290323 0.73290323 0.73419355\n",
      " 0.73548387 0.73419355 0.71741935 0.72516129 0.72516129 0.72645161\n",
      " 0.73419355 0.73677419 0.73548387 0.73419355 0.73548387 0.73548387\n",
      " 0.73419355 0.73677419 0.71870968 0.71870968 0.71741935 0.71741935\n",
      " 0.71354839 0.71741935 0.71612903 0.71483871 0.71354839 0.71741935\n",
      " 0.71612903 0.71483871 0.71354839 0.71483871 0.71612903 0.71483871\n",
      " 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645\n",
      " 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645\n",
      " 0.73806452 0.73548387 0.73548387 0.73419355 0.7316129  0.73548387\n",
      " 0.73290323 0.73290323 0.71225806 0.71483871 0.71483871 0.71354839\n",
      " 0.7316129  0.7316129  0.73419355 0.7316129  0.73032258 0.73032258\n",
      " 0.73032258 0.7316129  0.71483871 0.71612903 0.71225806 0.71096774\n",
      " 0.71225806 0.70967742 0.7083871  0.7083871  0.71225806 0.70967742\n",
      " 0.7083871  0.7083871  0.70967742 0.70967742 0.7083871  0.70967742\n",
      " 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613\n",
      " 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.73290323 0.7316129  0.7316129  0.73290323 0.73290323 0.73419355\n",
      " 0.73548387 0.73419355 0.71741935 0.72516129 0.72516129 0.72645161\n",
      " 0.73419355 0.73677419 0.73548387 0.73419355 0.73548387 0.73548387\n",
      " 0.73419355 0.73677419 0.71870968 0.71870968 0.71741935 0.71741935\n",
      " 0.71354839 0.71741935 0.71612903 0.71483871 0.71354839 0.71741935\n",
      " 0.71612903 0.71483871 0.71354839 0.71483871 0.71612903 0.71483871\n",
      " 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645\n",
      " 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645\n",
      " 0.73806452 0.73548387 0.73548387 0.73419355 0.7316129  0.73548387\n",
      " 0.73290323 0.73290323 0.71225806 0.71483871 0.71483871 0.71354839\n",
      " 0.7316129  0.7316129  0.73419355 0.7316129  0.73032258 0.73032258\n",
      " 0.73032258 0.7316129  0.71483871 0.71612903 0.71225806 0.71096774\n",
      " 0.71225806 0.70967742 0.7083871  0.7083871  0.71225806 0.70967742\n",
      " 0.7083871  0.7083871  0.70967742 0.70967742 0.7083871  0.70967742\n",
      " 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613\n",
      " 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.73290323 0.7316129  0.7316129  0.73290323 0.73290323 0.73419355\n",
      " 0.73548387 0.73419355 0.71741935 0.72516129 0.72516129 0.72645161\n",
      " 0.73419355 0.73677419 0.73548387 0.73419355 0.73548387 0.73548387\n",
      " 0.73419355 0.73677419 0.71870968 0.71870968 0.71741935 0.71741935\n",
      " 0.71354839 0.71741935 0.71612903 0.71483871 0.71354839 0.71741935\n",
      " 0.71612903 0.71483871 0.71354839 0.71483871 0.71612903 0.71483871\n",
      " 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645\n",
      " 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645 0.70580645\n",
      " 0.73806452 0.73548387 0.73548387 0.73419355 0.7316129  0.73548387\n",
      " 0.73290323 0.73290323 0.71225806 0.71483871 0.71483871 0.71354839\n",
      " 0.7316129  0.7316129  0.73419355 0.7316129  0.73032258 0.73032258\n",
      " 0.73032258 0.7316129  0.71483871 0.71612903 0.71225806 0.71096774\n",
      " 0.71225806 0.70967742 0.7083871  0.7083871  0.71225806 0.70967742\n",
      " 0.7083871  0.7083871  0.70967742 0.70967742 0.7083871  0.70967742\n",
      " 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613\n",
      " 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613 0.70451613]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Book]: 0.722007722007722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71741935 0.72       0.71612903 0.71483871 0.71741935 0.71870968\n",
      " 0.71870968 0.72       0.71354839 0.7083871  0.71096774 0.7083871\n",
      " 0.72387097 0.72516129 0.72129032 0.72129032 0.72258065 0.72129032\n",
      " 0.71741935 0.71612903 0.70967742 0.70709677 0.7083871  0.7083871\n",
      " 0.70064516 0.70193548 0.70322581 0.69935484 0.70064516 0.70193548\n",
      " 0.70322581 0.69935484 0.70064516 0.70322581 0.70322581 0.70193548\n",
      " 0.68258065 0.68129032 0.68129032 0.68129032 0.68258065 0.68129032\n",
      " 0.68129032 0.68129032 0.68258065 0.68129032 0.68129032 0.68129032\n",
      " 0.72129032 0.71870968 0.71612903 0.71483871 0.71741935 0.71741935\n",
      " 0.71741935 0.71741935 0.69935484 0.69677419 0.69806452 0.69935484\n",
      " 0.71483871 0.71354839 0.71483871 0.71354839 0.71225806 0.71483871\n",
      " 0.71354839 0.71225806 0.70322581 0.70064516 0.69677419 0.69806452\n",
      " 0.69290323 0.69290323 0.69290323 0.6916129  0.69290323 0.69290323\n",
      " 0.69290323 0.6916129  0.69032258 0.6916129  0.69290323 0.6916129\n",
      " 0.68129032 0.68       0.67870968 0.67870968 0.68129032 0.68\n",
      " 0.67870968 0.67870968 0.68129032 0.68       0.67870968 0.67870968\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71225806 0.71483871 0.71612903 0.71870968 0.71354839 0.71096774\n",
      " 0.71741935 0.71870968 0.70451613 0.70580645 0.70709677 0.7083871\n",
      " 0.72       0.72129032 0.72258065 0.72       0.71354839 0.71483871\n",
      " 0.71354839 0.71483871 0.70967742 0.70709677 0.70451613 0.70580645\n",
      " 0.69935484 0.69935484 0.69935484 0.69677419 0.69935484 0.69935484\n",
      " 0.69935484 0.69677419 0.69935484 0.69806452 0.69935484 0.69935484\n",
      " 0.68387097 0.68258065 0.68258065 0.68129032 0.68387097 0.68258065\n",
      " 0.68258065 0.68129032 0.68387097 0.68258065 0.68258065 0.68129032\n",
      " 0.71096774 0.71225806 0.71096774 0.71096774 0.70967742 0.71483871\n",
      " 0.71354839 0.71354839 0.70064516 0.69935484 0.69935484 0.69935484\n",
      " 0.71096774 0.71096774 0.70967742 0.7083871  0.70967742 0.70967742\n",
      " 0.71225806 0.70967742 0.70064516 0.69419355 0.69290323 0.69419355\n",
      " 0.6916129  0.69290323 0.69290323 0.6916129  0.6916129  0.69290323\n",
      " 0.69290323 0.6916129  0.69290323 0.6916129  0.69032258 0.69032258\n",
      " 0.68129032 0.68129032 0.67870968 0.67870968 0.68129032 0.68129032\n",
      " 0.67870968 0.67870968 0.68129032 0.68129032 0.67870968 0.67870968\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71741935 0.72129032 0.71612903 0.71483871 0.71741935 0.71870968\n",
      " 0.71870968 0.72       0.71354839 0.7083871  0.71096774 0.7083871\n",
      " 0.72387097 0.72516129 0.72129032 0.72129032 0.72258065 0.72129032\n",
      " 0.71741935 0.71612903 0.70967742 0.70709677 0.7083871  0.7083871\n",
      " 0.70064516 0.70193548 0.70322581 0.69935484 0.70064516 0.70193548\n",
      " 0.70322581 0.69935484 0.70064516 0.70322581 0.70322581 0.70193548\n",
      " 0.68258065 0.68129032 0.68129032 0.68129032 0.68258065 0.68129032\n",
      " 0.68129032 0.68129032 0.68258065 0.68129032 0.68129032 0.68129032\n",
      " 0.72129032 0.71870968 0.71612903 0.71483871 0.71741935 0.71741935\n",
      " 0.71741935 0.71741935 0.69935484 0.69677419 0.69806452 0.69935484\n",
      " 0.71483871 0.71354839 0.71483871 0.71354839 0.71225806 0.71483871\n",
      " 0.71354839 0.71225806 0.70322581 0.70064516 0.69677419 0.69806452\n",
      " 0.69290323 0.69290323 0.69290323 0.6916129  0.69290323 0.69290323\n",
      " 0.69290323 0.6916129  0.69032258 0.6916129  0.69290323 0.6916129\n",
      " 0.68129032 0.68       0.67870968 0.67870968 0.68129032 0.68\n",
      " 0.67870968 0.67870968 0.68129032 0.68       0.67870968 0.67870968\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71741935 0.72       0.71612903 0.71483871 0.71741935 0.71870968\n",
      " 0.71870968 0.72       0.71354839 0.7083871  0.71096774 0.7083871\n",
      " 0.72387097 0.72516129 0.72129032 0.72129032 0.72258065 0.72129032\n",
      " 0.71741935 0.71612903 0.70967742 0.70709677 0.7083871  0.7083871\n",
      " 0.70064516 0.70193548 0.70322581 0.69935484 0.70064516 0.70193548\n",
      " 0.70322581 0.69935484 0.70064516 0.70322581 0.70322581 0.70193548\n",
      " 0.68258065 0.68129032 0.68129032 0.68129032 0.68258065 0.68129032\n",
      " 0.68129032 0.68129032 0.68258065 0.68129032 0.68129032 0.68129032\n",
      " 0.72129032 0.71870968 0.71612903 0.71483871 0.71741935 0.71741935\n",
      " 0.71741935 0.71741935 0.69935484 0.69677419 0.69806452 0.69935484\n",
      " 0.71483871 0.71354839 0.71483871 0.71354839 0.71225806 0.71483871\n",
      " 0.71354839 0.71225806 0.70322581 0.70064516 0.69677419 0.69806452\n",
      " 0.69290323 0.69290323 0.69290323 0.6916129  0.69290323 0.69290323\n",
      " 0.69290323 0.6916129  0.69032258 0.6916129  0.69290323 0.6916129\n",
      " 0.68129032 0.68       0.67870968 0.67870968 0.68129032 0.68\n",
      " 0.67870968 0.67870968 0.68129032 0.68       0.67870968 0.67870968\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71741935 0.72       0.71612903 0.71483871 0.71741935 0.71870968\n",
      " 0.71870968 0.72       0.71354839 0.7083871  0.71096774 0.7083871\n",
      " 0.72387097 0.72516129 0.72129032 0.72129032 0.72258065 0.72129032\n",
      " 0.71741935 0.71612903 0.70967742 0.70709677 0.7083871  0.7083871\n",
      " 0.70064516 0.70193548 0.70322581 0.69935484 0.70064516 0.70193548\n",
      " 0.70322581 0.69935484 0.70064516 0.70322581 0.70322581 0.70193548\n",
      " 0.68258065 0.68129032 0.68129032 0.68129032 0.68258065 0.68129032\n",
      " 0.68129032 0.68129032 0.68258065 0.68129032 0.68129032 0.68129032\n",
      " 0.72129032 0.71870968 0.71612903 0.71483871 0.71741935 0.71741935\n",
      " 0.71741935 0.71741935 0.69935484 0.69677419 0.69806452 0.69935484\n",
      " 0.71483871 0.71354839 0.71483871 0.71354839 0.71225806 0.71483871\n",
      " 0.71354839 0.71225806 0.70322581 0.70064516 0.69677419 0.69806452\n",
      " 0.69290323 0.69290323 0.69290323 0.6916129  0.69290323 0.69290323\n",
      " 0.69290323 0.6916129  0.69032258 0.6916129  0.69290323 0.6916129\n",
      " 0.68129032 0.68       0.67870968 0.67870968 0.68129032 0.68\n",
      " 0.67870968 0.67870968 0.68129032 0.68       0.67870968 0.67870968]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Lecture Note]: 0.7027027027027027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67354839 0.68       0.68258065 0.67870968 0.67612903 0.68\n",
      " 0.68774194 0.68774194 0.66967742 0.67612903 0.67870968 0.67870968\n",
      " 0.67870968 0.67612903 0.67870968 0.68258065 0.67870968 0.68258065\n",
      " 0.68258065 0.68387097 0.67096774 0.67483871 0.67870968 0.68\n",
      " 0.67354839 0.68129032 0.68129032 0.68258065 0.67354839 0.68129032\n",
      " 0.68129032 0.68258065 0.67483871 0.67483871 0.66580645 0.6683871\n",
      " 0.66193548 0.66193548 0.66193548 0.66451613 0.66193548 0.66193548\n",
      " 0.66193548 0.66451613 0.66193548 0.66193548 0.66193548 0.66451613\n",
      " 0.67870968 0.67870968 0.68258065 0.68258065 0.67483871 0.67741935\n",
      " 0.68       0.68       0.67741935 0.68       0.67612903 0.67483871\n",
      " 0.68516129 0.68516129 0.68645161 0.68645161 0.68129032 0.68258065\n",
      " 0.68387097 0.68516129 0.67612903 0.67483871 0.67483871 0.67612903\n",
      " 0.66580645 0.6683871  0.66709677 0.66709677 0.66580645 0.6683871\n",
      " 0.66709677 0.66709677 0.6683871  0.6683871  0.6683871  0.67096774\n",
      " 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516\n",
      " 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67612903 0.68129032 0.68129032 0.68258065 0.68129032 0.68258065\n",
      " 0.68258065 0.68       0.67096774 0.67483871 0.68129032 0.68258065\n",
      " 0.66967742 0.67354839 0.68258065 0.68129032 0.67096774 0.68129032\n",
      " 0.68645161 0.68516129 0.67225806 0.67612903 0.68258065 0.68387097\n",
      " 0.67870968 0.67870968 0.67741935 0.67612903 0.67870968 0.67870968\n",
      " 0.67741935 0.67612903 0.67870968 0.67225806 0.6683871  0.6683871\n",
      " 0.66193548 0.66193548 0.66193548 0.66451613 0.66193548 0.66193548\n",
      " 0.66193548 0.66451613 0.66193548 0.66193548 0.66193548 0.66451613\n",
      " 0.68129032 0.68258065 0.68774194 0.68516129 0.68129032 0.68258065\n",
      " 0.68516129 0.68516129 0.67354839 0.67870968 0.67741935 0.67612903\n",
      " 0.68258065 0.68129032 0.68387097 0.68387097 0.68129032 0.68387097\n",
      " 0.68516129 0.68645161 0.67354839 0.67225806 0.67225806 0.66967742\n",
      " 0.66451613 0.66580645 0.6683871  0.66967742 0.66451613 0.66580645\n",
      " 0.6683871  0.66967742 0.66580645 0.6683871  0.6683871  0.66967742\n",
      " 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516\n",
      " 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67354839 0.68       0.68258065 0.67870968 0.67612903 0.68\n",
      " 0.68774194 0.68774194 0.66967742 0.67612903 0.67870968 0.67870968\n",
      " 0.67870968 0.67612903 0.67870968 0.68258065 0.67870968 0.68258065\n",
      " 0.68258065 0.68387097 0.67096774 0.67483871 0.67870968 0.68\n",
      " 0.67354839 0.68129032 0.68129032 0.68258065 0.67354839 0.68129032\n",
      " 0.68129032 0.68258065 0.67483871 0.67483871 0.66580645 0.6683871\n",
      " 0.66193548 0.66193548 0.66193548 0.66451613 0.66193548 0.66193548\n",
      " 0.66193548 0.66451613 0.66193548 0.66193548 0.66193548 0.66451613\n",
      " 0.67870968 0.67870968 0.68258065 0.68258065 0.67483871 0.67741935\n",
      " 0.68       0.68       0.67741935 0.68       0.67612903 0.67483871\n",
      " 0.68516129 0.68516129 0.68645161 0.68645161 0.68129032 0.68258065\n",
      " 0.68387097 0.68516129 0.67612903 0.67483871 0.67483871 0.67612903\n",
      " 0.66580645 0.6683871  0.66709677 0.66709677 0.66580645 0.6683871\n",
      " 0.66709677 0.66709677 0.6683871  0.6683871  0.6683871  0.67096774\n",
      " 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516\n",
      " 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67354839 0.68       0.68258065 0.67870968 0.67612903 0.68\n",
      " 0.68774194 0.68774194 0.66967742 0.67612903 0.67870968 0.67870968\n",
      " 0.67870968 0.67612903 0.67870968 0.68258065 0.67870968 0.68258065\n",
      " 0.68258065 0.68387097 0.67096774 0.67483871 0.67870968 0.68\n",
      " 0.67354839 0.68129032 0.68129032 0.68258065 0.67354839 0.68129032\n",
      " 0.68129032 0.68258065 0.67483871 0.67483871 0.66580645 0.6683871\n",
      " 0.66193548 0.66193548 0.66193548 0.66451613 0.66193548 0.66193548\n",
      " 0.66193548 0.66451613 0.66193548 0.66193548 0.66193548 0.66451613\n",
      " 0.67870968 0.67870968 0.68258065 0.68258065 0.67483871 0.67741935\n",
      " 0.68       0.68       0.67741935 0.68       0.67612903 0.67483871\n",
      " 0.68516129 0.68516129 0.68645161 0.68645161 0.68129032 0.68258065\n",
      " 0.68387097 0.68516129 0.67612903 0.67483871 0.67483871 0.67612903\n",
      " 0.66580645 0.6683871  0.66709677 0.66709677 0.66580645 0.6683871\n",
      " 0.66709677 0.66709677 0.6683871  0.6683871  0.6683871  0.67096774\n",
      " 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516\n",
      " 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67354839 0.68       0.68258065 0.67870968 0.67612903 0.68\n",
      " 0.68774194 0.68774194 0.66967742 0.67612903 0.67870968 0.67870968\n",
      " 0.67870968 0.67612903 0.67870968 0.68258065 0.67870968 0.68258065\n",
      " 0.68258065 0.68387097 0.67096774 0.67483871 0.67870968 0.68\n",
      " 0.67354839 0.68129032 0.68129032 0.68258065 0.67354839 0.68129032\n",
      " 0.68129032 0.68258065 0.67483871 0.67483871 0.66580645 0.6683871\n",
      " 0.66193548 0.66193548 0.66193548 0.66451613 0.66193548 0.66193548\n",
      " 0.66193548 0.66451613 0.66193548 0.66193548 0.66193548 0.66451613\n",
      " 0.67870968 0.67870968 0.68258065 0.68258065 0.67483871 0.67741935\n",
      " 0.68       0.68       0.67741935 0.68       0.67612903 0.67483871\n",
      " 0.68516129 0.68516129 0.68645161 0.68645161 0.68129032 0.68258065\n",
      " 0.68387097 0.68516129 0.67612903 0.67483871 0.67483871 0.67612903\n",
      " 0.66580645 0.6683871  0.66709677 0.66709677 0.66580645 0.6683871\n",
      " 0.66709677 0.66709677 0.6683871  0.6683871  0.6683871  0.67096774\n",
      " 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516\n",
      " 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516 0.66064516]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Educational game]: 0.6486486486486487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56903226 0.55483871 0.57677419 0.58451613 0.5883871  0.59612903\n",
      " 0.59870968 0.60129032 0.60258065 0.60129032 0.59354839 0.59870968\n",
      " 0.59225806 0.59354839 0.60516129 0.60129032 0.57419355 0.57548387\n",
      " 0.58580645 0.60258065 0.5883871  0.59354839 0.59741935 0.59870968\n",
      " 0.59354839 0.60774194 0.59612903 0.59612903 0.59354839 0.60774194\n",
      " 0.59612903 0.59612903 0.60258065 0.60645161 0.60645161 0.61677419\n",
      " 0.59225806 0.59483871 0.58193548 0.59354839 0.59225806 0.59483871\n",
      " 0.58193548 0.59354839 0.59225806 0.59483871 0.58193548 0.59354839\n",
      " 0.59354839 0.58967742 0.59225806 0.58451613 0.53548387 0.56645161\n",
      " 0.55870968 0.56645161 0.60774194 0.61935484 0.59870968 0.60903226\n",
      " 0.58193548 0.60645161 0.59741935 0.59225806 0.58451613 0.59483871\n",
      " 0.59096774 0.58580645 0.57806452 0.59612903 0.57806452 0.57806452\n",
      " 0.58451613 0.58709677 0.59483871 0.58193548 0.58451613 0.58709677\n",
      " 0.59483871 0.58193548 0.57290323 0.58064516 0.57548387 0.57548387\n",
      " 0.58064516 0.57290323 0.57419355 0.5716129  0.58064516 0.57290323\n",
      " 0.57419355 0.5716129  0.58064516 0.57290323 0.57419355 0.5716129\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59483871 0.59483871 0.58967742 0.6        0.57032258 0.56129032\n",
      " 0.57419355 0.58709677 0.59483871 0.57548387 0.59096774 0.59870968\n",
      " 0.58967742 0.58451613 0.6        0.59225806 0.5716129  0.5716129\n",
      " 0.59096774 0.60258065 0.59225806 0.58322581 0.5883871  0.59096774\n",
      " 0.61548387 0.60258065 0.6        0.58709677 0.61548387 0.60258065\n",
      " 0.6        0.58709677 0.59096774 0.59870968 0.60516129 0.61419355\n",
      " 0.58451613 0.58322581 0.57806452 0.58967742 0.58451613 0.58322581\n",
      " 0.57806452 0.58967742 0.58451613 0.58322581 0.57806452 0.58967742\n",
      " 0.59096774 0.59225806 0.5883871  0.59096774 0.59225806 0.58193548\n",
      " 0.57290323 0.58580645 0.58451613 0.57548387 0.56645161 0.56516129\n",
      " 0.59096774 0.59483871 0.59225806 0.59096774 0.57290323 0.57806452\n",
      " 0.5716129  0.58451613 0.5716129  0.57290323 0.57419355 0.57935484\n",
      " 0.58193548 0.58322581 0.57548387 0.59483871 0.58193548 0.58322581\n",
      " 0.57548387 0.59483871 0.57677419 0.58967742 0.57806452 0.5883871\n",
      " 0.57935484 0.5716129  0.57419355 0.56774194 0.57935484 0.5716129\n",
      " 0.57419355 0.56774194 0.57935484 0.5716129  0.57419355 0.56774194\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56903226 0.55483871 0.57677419 0.58451613 0.5883871  0.59612903\n",
      " 0.59870968 0.60129032 0.60258065 0.60129032 0.59354839 0.59870968\n",
      " 0.59225806 0.59354839 0.60516129 0.60129032 0.57419355 0.57548387\n",
      " 0.58580645 0.60258065 0.5883871  0.59354839 0.59741935 0.59870968\n",
      " 0.59354839 0.60774194 0.59612903 0.59612903 0.59354839 0.60774194\n",
      " 0.59612903 0.59612903 0.60258065 0.60645161 0.60645161 0.61677419\n",
      " 0.59225806 0.59483871 0.58193548 0.59354839 0.59225806 0.59483871\n",
      " 0.58193548 0.59354839 0.59225806 0.59483871 0.58193548 0.59354839\n",
      " 0.59354839 0.59096774 0.58967742 0.58967742 0.53677419 0.56645161\n",
      " 0.55870968 0.56645161 0.60774194 0.61935484 0.59870968 0.60903226\n",
      " 0.58193548 0.60645161 0.59741935 0.59225806 0.58451613 0.59225806\n",
      " 0.59096774 0.58580645 0.57806452 0.59612903 0.57806452 0.57806452\n",
      " 0.58451613 0.58709677 0.59483871 0.58193548 0.58451613 0.58709677\n",
      " 0.59483871 0.58193548 0.57290323 0.58064516 0.57548387 0.57548387\n",
      " 0.58064516 0.57290323 0.57419355 0.5716129  0.58064516 0.57290323\n",
      " 0.57419355 0.5716129  0.58064516 0.57290323 0.57419355 0.5716129\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56903226 0.55483871 0.57677419 0.58451613 0.5883871  0.59612903\n",
      " 0.59870968 0.60129032 0.60258065 0.60129032 0.59354839 0.59870968\n",
      " 0.59225806 0.59354839 0.60516129 0.60129032 0.57419355 0.57548387\n",
      " 0.58580645 0.60258065 0.5883871  0.59354839 0.59741935 0.59870968\n",
      " 0.59354839 0.60774194 0.59612903 0.59612903 0.59354839 0.60774194\n",
      " 0.59612903 0.59612903 0.60258065 0.60645161 0.60645161 0.61677419\n",
      " 0.59225806 0.59483871 0.58193548 0.59354839 0.59225806 0.59483871\n",
      " 0.58193548 0.59354839 0.59225806 0.59483871 0.58193548 0.59354839\n",
      " 0.59354839 0.58967742 0.59225806 0.58451613 0.53548387 0.56645161\n",
      " 0.55870968 0.56645161 0.60774194 0.61935484 0.59870968 0.60903226\n",
      " 0.58193548 0.60645161 0.59741935 0.59225806 0.58451613 0.59483871\n",
      " 0.59096774 0.58580645 0.57806452 0.59612903 0.57806452 0.57806452\n",
      " 0.58451613 0.58709677 0.59483871 0.58193548 0.58451613 0.58709677\n",
      " 0.59483871 0.58193548 0.57290323 0.58064516 0.57548387 0.57548387\n",
      " 0.58064516 0.57290323 0.57419355 0.5716129  0.58064516 0.57290323\n",
      " 0.57419355 0.5716129  0.58064516 0.57290323 0.57419355 0.5716129\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56903226 0.55483871 0.57677419 0.58451613 0.5883871  0.59612903\n",
      " 0.59870968 0.60129032 0.60258065 0.60129032 0.59354839 0.59870968\n",
      " 0.59225806 0.59354839 0.60516129 0.60129032 0.57419355 0.57548387\n",
      " 0.58580645 0.60258065 0.5883871  0.59354839 0.59741935 0.59870968\n",
      " 0.59354839 0.60774194 0.59612903 0.59612903 0.59354839 0.60774194\n",
      " 0.59612903 0.59612903 0.60258065 0.60645161 0.60645161 0.61677419\n",
      " 0.59225806 0.59483871 0.58193548 0.59354839 0.59225806 0.59483871\n",
      " 0.58193548 0.59354839 0.59225806 0.59483871 0.58193548 0.59354839\n",
      " 0.59354839 0.58967742 0.59225806 0.58451613 0.53548387 0.56645161\n",
      " 0.55870968 0.56645161 0.60774194 0.61935484 0.59870968 0.60903226\n",
      " 0.58193548 0.60645161 0.59741935 0.59225806 0.58451613 0.59483871\n",
      " 0.59096774 0.58580645 0.57806452 0.59612903 0.57806452 0.57806452\n",
      " 0.58451613 0.58709677 0.59483871 0.58193548 0.58451613 0.58709677\n",
      " 0.59483871 0.58193548 0.57290323 0.58064516 0.57548387 0.57548387\n",
      " 0.58064516 0.57290323 0.57419355 0.5716129  0.58064516 0.57290323\n",
      " 0.57419355 0.5716129  0.58064516 0.57290323 0.57419355 0.5716129 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Video]: 0.6061776061776062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69290323 0.70064516 0.69806452 0.70193548 0.69419355 0.68774194\n",
      " 0.69290323 0.69290323 0.68387097 0.68903226 0.68774194 0.68774194\n",
      " 0.68903226 0.6916129  0.69548387 0.69419355 0.69677419 0.68903226\n",
      " 0.6916129  0.6916129  0.67225806 0.67870968 0.68       0.68129032\n",
      " 0.67741935 0.67741935 0.68       0.67870968 0.67741935 0.67741935\n",
      " 0.68       0.67870968 0.67096774 0.67096774 0.67354839 0.67483871\n",
      " 0.66451613 0.66709677 0.66709677 0.66322581 0.66451613 0.66709677\n",
      " 0.66709677 0.66322581 0.66451613 0.66709677 0.66709677 0.66322581\n",
      " 0.69290323 0.69290323 0.69032258 0.69677419 0.68258065 0.69419355\n",
      " 0.69548387 0.69548387 0.67741935 0.68129032 0.68129032 0.68129032\n",
      " 0.69548387 0.69032258 0.69032258 0.69419355 0.69290323 0.6916129\n",
      " 0.69419355 0.69548387 0.67354839 0.67096774 0.67612903 0.67612903\n",
      " 0.67096774 0.67354839 0.67483871 0.67483871 0.67096774 0.67354839\n",
      " 0.67483871 0.67483871 0.66580645 0.67096774 0.67483871 0.67354839\n",
      " 0.65677419 0.65677419 0.65548387 0.65548387 0.65677419 0.65677419\n",
      " 0.65548387 0.65548387 0.65677419 0.65677419 0.65548387 0.65548387\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69290323 0.69677419 0.69677419 0.69806452 0.68903226 0.6916129\n",
      " 0.68903226 0.69290323 0.68774194 0.6916129  0.69032258 0.6916129\n",
      " 0.6916129  0.69290323 0.6916129  0.69548387 0.69548387 0.6916129\n",
      " 0.70064516 0.69419355 0.67870968 0.68129032 0.68       0.68129032\n",
      " 0.67612903 0.68       0.67612903 0.67483871 0.67612903 0.68\n",
      " 0.67612903 0.67483871 0.67612903 0.67483871 0.67612903 0.67483871\n",
      " 0.66580645 0.66580645 0.66580645 0.66322581 0.66580645 0.66580645\n",
      " 0.66580645 0.66322581 0.66580645 0.66580645 0.66580645 0.66322581\n",
      " 0.70064516 0.69290323 0.69290323 0.6916129  0.69032258 0.69032258\n",
      " 0.69290323 0.6916129  0.67483871 0.67483871 0.67741935 0.68129032\n",
      " 0.6916129  0.68774194 0.6916129  0.6916129  0.68774194 0.6916129\n",
      " 0.6916129  0.69419355 0.67483871 0.67096774 0.67096774 0.67354839\n",
      " 0.67096774 0.67096774 0.67354839 0.67354839 0.67096774 0.67096774\n",
      " 0.67354839 0.67354839 0.67354839 0.66967742 0.67483871 0.67354839\n",
      " 0.65548387 0.65677419 0.65548387 0.65548387 0.65548387 0.65677419\n",
      " 0.65548387 0.65548387 0.65548387 0.65677419 0.65548387 0.65548387\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69290323 0.70064516 0.69806452 0.70193548 0.69419355 0.68774194\n",
      " 0.69290323 0.6916129  0.68387097 0.68903226 0.68774194 0.68774194\n",
      " 0.68903226 0.6916129  0.69548387 0.69419355 0.69677419 0.68903226\n",
      " 0.6916129  0.6916129  0.67225806 0.67870968 0.68       0.68129032\n",
      " 0.67741935 0.67741935 0.68       0.67870968 0.67741935 0.67741935\n",
      " 0.68       0.67870968 0.67096774 0.67096774 0.67354839 0.67483871\n",
      " 0.66451613 0.66709677 0.66709677 0.66322581 0.66451613 0.66709677\n",
      " 0.66709677 0.66322581 0.66451613 0.66709677 0.66709677 0.66322581\n",
      " 0.69290323 0.69032258 0.69032258 0.69548387 0.68258065 0.69419355\n",
      " 0.69548387 0.69548387 0.67741935 0.68129032 0.68129032 0.68129032\n",
      " 0.69548387 0.69032258 0.69032258 0.69419355 0.69290323 0.6916129\n",
      " 0.69419355 0.69548387 0.67354839 0.67096774 0.67612903 0.67612903\n",
      " 0.67096774 0.67354839 0.67483871 0.67483871 0.67096774 0.67354839\n",
      " 0.67483871 0.67483871 0.66580645 0.67096774 0.67483871 0.67354839\n",
      " 0.65677419 0.65677419 0.65548387 0.65548387 0.65677419 0.65677419\n",
      " 0.65548387 0.65548387 0.65677419 0.65677419 0.65548387 0.65548387\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69290323 0.70064516 0.69806452 0.70193548 0.69419355 0.68774194\n",
      " 0.69290323 0.69290323 0.68387097 0.68903226 0.68774194 0.68774194\n",
      " 0.68903226 0.6916129  0.69548387 0.69419355 0.69677419 0.68903226\n",
      " 0.6916129  0.6916129  0.67225806 0.67870968 0.68       0.68129032\n",
      " 0.67741935 0.67741935 0.68       0.67870968 0.67741935 0.67741935\n",
      " 0.68       0.67870968 0.67096774 0.67096774 0.67354839 0.67483871\n",
      " 0.66451613 0.66709677 0.66709677 0.66322581 0.66451613 0.66709677\n",
      " 0.66709677 0.66322581 0.66451613 0.66709677 0.66709677 0.66322581\n",
      " 0.69290323 0.69290323 0.69032258 0.69677419 0.68258065 0.69419355\n",
      " 0.69548387 0.69548387 0.67741935 0.68129032 0.68129032 0.68129032\n",
      " 0.69548387 0.69032258 0.69032258 0.69419355 0.69290323 0.6916129\n",
      " 0.69419355 0.69548387 0.67354839 0.67096774 0.67612903 0.67612903\n",
      " 0.67096774 0.67354839 0.67483871 0.67483871 0.67096774 0.67354839\n",
      " 0.67483871 0.67483871 0.66580645 0.67096774 0.67483871 0.67354839\n",
      " 0.65677419 0.65677419 0.65548387 0.65548387 0.65677419 0.65677419\n",
      " 0.65548387 0.65548387 0.65677419 0.65677419 0.65548387 0.65548387\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69290323 0.70064516 0.69806452 0.70193548 0.69419355 0.68774194\n",
      " 0.69290323 0.69290323 0.68387097 0.68903226 0.68774194 0.68774194\n",
      " 0.68903226 0.6916129  0.69548387 0.69419355 0.69677419 0.68903226\n",
      " 0.6916129  0.6916129  0.67225806 0.67870968 0.68       0.68129032\n",
      " 0.67741935 0.67741935 0.68       0.67870968 0.67741935 0.67741935\n",
      " 0.68       0.67870968 0.67096774 0.67096774 0.67354839 0.67483871\n",
      " 0.66451613 0.66709677 0.66709677 0.66322581 0.66451613 0.66709677\n",
      " 0.66709677 0.66322581 0.66451613 0.66709677 0.66709677 0.66322581\n",
      " 0.69290323 0.69290323 0.69032258 0.69677419 0.68258065 0.69419355\n",
      " 0.69548387 0.69548387 0.67741935 0.68129032 0.68129032 0.68129032\n",
      " 0.69548387 0.69032258 0.69032258 0.69419355 0.69290323 0.6916129\n",
      " 0.69419355 0.69548387 0.67354839 0.67096774 0.67612903 0.67612903\n",
      " 0.67096774 0.67354839 0.67483871 0.67483871 0.67096774 0.67354839\n",
      " 0.67483871 0.67483871 0.66580645 0.67096774 0.67483871 0.67354839\n",
      " 0.65677419 0.65677419 0.65548387 0.65548387 0.65677419 0.65677419\n",
      " 0.65548387 0.65548387 0.65677419 0.65677419 0.65548387 0.65548387]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Audio-recorded lecture]: 0.7065637065637066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64645161 0.65419355 0.64645161 0.64774194 0.63870968 0.64387097\n",
      " 0.64516129 0.64258065 0.64645161 0.64516129 0.64258065 0.64645161\n",
      " 0.64129032 0.64903226 0.65032258 0.65032258 0.64387097 0.65290323\n",
      " 0.64903226 0.6516129  0.64774194 0.65032258 0.65032258 0.64774194\n",
      " 0.64       0.64129032 0.63741935 0.64387097 0.64       0.64129032\n",
      " 0.63741935 0.64387097 0.64774194 0.64387097 0.64774194 0.64516129\n",
      " 0.63612903 0.64       0.63870968 0.63741935 0.63612903 0.64\n",
      " 0.63870968 0.63741935 0.63612903 0.64       0.63870968 0.63741935\n",
      " 0.65806452 0.66064516 0.65677419 0.65677419 0.64903226 0.6516129\n",
      " 0.65677419 0.65677419 0.64516129 0.65032258 0.6516129  0.65290323\n",
      " 0.64774194 0.65290323 0.6516129  0.65806452 0.65935484 0.65806452\n",
      " 0.65677419 0.66193548 0.64903226 0.64387097 0.64774194 0.64903226\n",
      " 0.64       0.64129032 0.64258065 0.64516129 0.64       0.64129032\n",
      " 0.64258065 0.64516129 0.64516129 0.64387097 0.64516129 0.64387097\n",
      " 0.63870968 0.64       0.64       0.64       0.63870968 0.64\n",
      " 0.64       0.64       0.63870968 0.64       0.64       0.64\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64645161 0.6516129  0.65032258 0.64645161 0.64903226 0.65806452\n",
      " 0.65677419 0.65032258 0.64129032 0.64387097 0.64129032 0.64516129\n",
      " 0.65677419 0.65677419 0.66064516 0.65806452 0.64903226 0.64258065\n",
      " 0.65032258 0.65032258 0.63870968 0.64129032 0.64516129 0.64903226\n",
      " 0.64774194 0.64903226 0.64129032 0.64645161 0.64774194 0.64903226\n",
      " 0.64129032 0.64645161 0.64258065 0.64258065 0.64516129 0.64516129\n",
      " 0.63741935 0.63870968 0.64       0.63870968 0.63741935 0.63870968\n",
      " 0.64       0.63870968 0.63741935 0.63870968 0.64       0.63870968\n",
      " 0.65419355 0.65032258 0.65548387 0.65032258 0.6516129  0.65419355\n",
      " 0.65032258 0.65677419 0.64645161 0.65032258 0.65032258 0.64774194\n",
      " 0.6516129  0.65935484 0.65935484 0.66064516 0.6516129  0.65419355\n",
      " 0.65290323 0.65548387 0.64645161 0.64645161 0.64903226 0.64903226\n",
      " 0.63870968 0.64       0.64387097 0.64258065 0.63870968 0.64\n",
      " 0.64387097 0.64258065 0.64645161 0.64774194 0.64774194 0.64774194\n",
      " 0.64       0.64       0.64       0.64       0.64       0.64\n",
      " 0.64       0.64       0.64       0.64       0.64       0.64\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64645161 0.65419355 0.64774194 0.64645161 0.63870968 0.64387097\n",
      " 0.64516129 0.64258065 0.64645161 0.64516129 0.64258065 0.64645161\n",
      " 0.64129032 0.64903226 0.65032258 0.65032258 0.64387097 0.65290323\n",
      " 0.64903226 0.6516129  0.64774194 0.65032258 0.65032258 0.64774194\n",
      " 0.64       0.64129032 0.63741935 0.64387097 0.64       0.64129032\n",
      " 0.63741935 0.64387097 0.64774194 0.64387097 0.64774194 0.64516129\n",
      " 0.63612903 0.64       0.63870968 0.63741935 0.63612903 0.64\n",
      " 0.63870968 0.63741935 0.63612903 0.64       0.63870968 0.63741935\n",
      " 0.65806452 0.65935484 0.65806452 0.65677419 0.64903226 0.6516129\n",
      " 0.65677419 0.65677419 0.64516129 0.65032258 0.6516129  0.65290323\n",
      " 0.64774194 0.65290323 0.6516129  0.65806452 0.65935484 0.65806452\n",
      " 0.65677419 0.66193548 0.64903226 0.64387097 0.64774194 0.64903226\n",
      " 0.64       0.64129032 0.64258065 0.64516129 0.64       0.64129032\n",
      " 0.64258065 0.64516129 0.64516129 0.64387097 0.64516129 0.64387097\n",
      " 0.63870968 0.64       0.64       0.64       0.63870968 0.64\n",
      " 0.64       0.64       0.63870968 0.64       0.64       0.64\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64645161 0.65419355 0.64645161 0.64774194 0.63870968 0.64387097\n",
      " 0.64516129 0.64258065 0.64645161 0.64516129 0.64258065 0.64645161\n",
      " 0.64129032 0.64903226 0.65032258 0.65032258 0.64387097 0.65290323\n",
      " 0.64903226 0.6516129  0.64774194 0.65032258 0.65032258 0.64774194\n",
      " 0.64       0.64129032 0.63741935 0.64387097 0.64       0.64129032\n",
      " 0.63741935 0.64387097 0.64774194 0.64387097 0.64774194 0.64516129\n",
      " 0.63612903 0.64       0.63870968 0.63741935 0.63612903 0.64\n",
      " 0.63870968 0.63741935 0.63612903 0.64       0.63870968 0.63741935\n",
      " 0.65806452 0.66064516 0.65677419 0.65677419 0.64903226 0.6516129\n",
      " 0.65677419 0.65677419 0.64516129 0.65032258 0.6516129  0.65290323\n",
      " 0.64774194 0.65290323 0.6516129  0.65806452 0.65935484 0.65806452\n",
      " 0.65677419 0.66193548 0.64903226 0.64387097 0.64774194 0.64903226\n",
      " 0.64       0.64129032 0.64258065 0.64516129 0.64       0.64129032\n",
      " 0.64258065 0.64516129 0.64516129 0.64387097 0.64516129 0.64387097\n",
      " 0.63870968 0.64       0.64       0.64       0.63870968 0.64\n",
      " 0.64       0.64       0.63870968 0.64       0.64       0.64\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64645161 0.65419355 0.64645161 0.64774194 0.63870968 0.64387097\n",
      " 0.64516129 0.64258065 0.64645161 0.64516129 0.64258065 0.64645161\n",
      " 0.64129032 0.64903226 0.65032258 0.65032258 0.64387097 0.65290323\n",
      " 0.64903226 0.6516129  0.64774194 0.65032258 0.65032258 0.64774194\n",
      " 0.64       0.64129032 0.63741935 0.64387097 0.64       0.64129032\n",
      " 0.63741935 0.64387097 0.64774194 0.64387097 0.64774194 0.64516129\n",
      " 0.63612903 0.64       0.63870968 0.63741935 0.63612903 0.64\n",
      " 0.63870968 0.63741935 0.63612903 0.64       0.63870968 0.63741935\n",
      " 0.65806452 0.66064516 0.65677419 0.65677419 0.64903226 0.6516129\n",
      " 0.65677419 0.65677419 0.64516129 0.65032258 0.6516129  0.65290323\n",
      " 0.64774194 0.65290323 0.6516129  0.65806452 0.65935484 0.65806452\n",
      " 0.65677419 0.66193548 0.64903226 0.64387097 0.64774194 0.64903226\n",
      " 0.64       0.64129032 0.64258065 0.64516129 0.64       0.64129032\n",
      " 0.64258065 0.64516129 0.64516129 0.64387097 0.64516129 0.64387097\n",
      " 0.63870968 0.64       0.64       0.64       0.63870968 0.64\n",
      " 0.64       0.64       0.63870968 0.64       0.64       0.64      ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Animated instruction]: 0.5907335907335908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63741935 0.63354839 0.64387097 0.62967742 0.64387097 0.64258065\n",
      " 0.64903226 0.64129032 0.62967742 0.63225806 0.63354839 0.62322581\n",
      " 0.6283871  0.62451613 0.62322581 0.62709677 0.63741935 0.64258065\n",
      " 0.64903226 0.65290323 0.62064516 0.61806452 0.62580645 0.61806452\n",
      " 0.63612903 0.6283871  0.62193548 0.62322581 0.63612903 0.6283871\n",
      " 0.62193548 0.62322581 0.61935484 0.62451613 0.62322581 0.63096774\n",
      " 0.61419355 0.61419355 0.6116129  0.60774194 0.61419355 0.61419355\n",
      " 0.6116129  0.60774194 0.61419355 0.61419355 0.6116129  0.60774194\n",
      " 0.64774194 0.65290323 0.64516129 0.63870968 0.63612903 0.64387097\n",
      " 0.63354839 0.63612903 0.62967742 0.63483871 0.6283871  0.62451613\n",
      " 0.63096774 0.63483871 0.63354839 0.63096774 0.63354839 0.64\n",
      " 0.63483871 0.63483871 0.62580645 0.6283871  0.62322581 0.62193548\n",
      " 0.62322581 0.63096774 0.62451613 0.62193548 0.62322581 0.63096774\n",
      " 0.62451613 0.62193548 0.62451613 0.62709677 0.61935484 0.62322581\n",
      " 0.60903226 0.60387097 0.60903226 0.6116129  0.60903226 0.60387097\n",
      " 0.60903226 0.6116129  0.60903226 0.60387097 0.60903226 0.6116129\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63096774 0.64387097 0.64       0.63741935 0.61806452 0.62580645\n",
      " 0.63483871 0.63612903 0.6116129  0.62064516 0.63225806 0.63096774\n",
      " 0.63096774 0.62967742 0.63225806 0.63225806 0.63096774 0.64\n",
      " 0.64       0.63741935 0.61419355 0.61935484 0.62193548 0.61290323\n",
      " 0.62967742 0.61548387 0.61677419 0.62193548 0.62967742 0.61548387\n",
      " 0.61677419 0.62193548 0.62064516 0.61419355 0.62451613 0.63096774\n",
      " 0.61677419 0.61419355 0.60903226 0.60903226 0.61677419 0.61419355\n",
      " 0.60903226 0.60903226 0.61677419 0.61419355 0.60903226 0.60903226\n",
      " 0.62580645 0.6283871  0.63096774 0.62709677 0.64258065 0.63870968\n",
      " 0.64129032 0.64645161 0.62193548 0.62967742 0.63354839 0.6283871\n",
      " 0.63870968 0.63741935 0.63225806 0.63225806 0.64129032 0.63483871\n",
      " 0.63225806 0.63225806 0.61806452 0.62064516 0.61935484 0.62193548\n",
      " 0.62967742 0.62967742 0.63096774 0.62451613 0.62967742 0.62967742\n",
      " 0.63096774 0.62451613 0.62064516 0.62064516 0.62064516 0.62451613\n",
      " 0.60903226 0.60516129 0.61032258 0.61290323 0.60903226 0.60516129\n",
      " 0.61032258 0.61290323 0.60903226 0.60516129 0.61032258 0.61290323\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63741935 0.63354839 0.64387097 0.62967742 0.64387097 0.64258065\n",
      " 0.64903226 0.64129032 0.62967742 0.63225806 0.63354839 0.62322581\n",
      " 0.6283871  0.62451613 0.62322581 0.62709677 0.63741935 0.64258065\n",
      " 0.64903226 0.65290323 0.62064516 0.61806452 0.62580645 0.61806452\n",
      " 0.63612903 0.6283871  0.62193548 0.62322581 0.63612903 0.6283871\n",
      " 0.62193548 0.62322581 0.61935484 0.62451613 0.62322581 0.63096774\n",
      " 0.61419355 0.61419355 0.6116129  0.60774194 0.61419355 0.61419355\n",
      " 0.6116129  0.60774194 0.61419355 0.61419355 0.6116129  0.60774194\n",
      " 0.64774194 0.6516129  0.64774194 0.63870968 0.63741935 0.64387097\n",
      " 0.63354839 0.63612903 0.62967742 0.63483871 0.6283871  0.62451613\n",
      " 0.63096774 0.63483871 0.63354839 0.63096774 0.63354839 0.64\n",
      " 0.63483871 0.63483871 0.62580645 0.6283871  0.62322581 0.62193548\n",
      " 0.62322581 0.63096774 0.62451613 0.62193548 0.62322581 0.63096774\n",
      " 0.62451613 0.62193548 0.62451613 0.62709677 0.61935484 0.62322581\n",
      " 0.60903226 0.60387097 0.60903226 0.6116129  0.60903226 0.60387097\n",
      " 0.60903226 0.6116129  0.60903226 0.60387097 0.60903226 0.6116129\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63741935 0.63354839 0.64387097 0.62967742 0.64387097 0.64258065\n",
      " 0.64903226 0.64129032 0.62967742 0.63225806 0.63354839 0.62322581\n",
      " 0.6283871  0.62451613 0.62322581 0.62709677 0.63741935 0.64258065\n",
      " 0.64903226 0.65290323 0.62064516 0.61806452 0.62580645 0.61806452\n",
      " 0.63612903 0.6283871  0.62193548 0.62322581 0.63612903 0.6283871\n",
      " 0.62193548 0.62322581 0.61935484 0.62451613 0.62322581 0.63096774\n",
      " 0.61419355 0.61419355 0.6116129  0.60774194 0.61419355 0.61419355\n",
      " 0.6116129  0.60774194 0.61419355 0.61419355 0.6116129  0.60774194\n",
      " 0.64774194 0.65290323 0.64516129 0.63870968 0.63612903 0.64387097\n",
      " 0.63354839 0.63612903 0.62967742 0.63483871 0.6283871  0.62451613\n",
      " 0.63096774 0.63483871 0.63354839 0.63096774 0.63354839 0.64\n",
      " 0.63483871 0.63483871 0.62580645 0.6283871  0.62322581 0.62193548\n",
      " 0.62322581 0.63096774 0.62451613 0.62193548 0.62322581 0.63096774\n",
      " 0.62451613 0.62193548 0.62451613 0.62709677 0.61935484 0.62322581\n",
      " 0.60903226 0.60387097 0.60903226 0.6116129  0.60903226 0.60387097\n",
      " 0.60903226 0.6116129  0.60903226 0.60387097 0.60903226 0.6116129\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63741935 0.63354839 0.64387097 0.62967742 0.64387097 0.64258065\n",
      " 0.64903226 0.64129032 0.62967742 0.63225806 0.63354839 0.62322581\n",
      " 0.6283871  0.62451613 0.62322581 0.62709677 0.63741935 0.64258065\n",
      " 0.64903226 0.65290323 0.62064516 0.61806452 0.62580645 0.61806452\n",
      " 0.63612903 0.6283871  0.62193548 0.62322581 0.63612903 0.6283871\n",
      " 0.62193548 0.62322581 0.61935484 0.62451613 0.62322581 0.63096774\n",
      " 0.61419355 0.61419355 0.6116129  0.60774194 0.61419355 0.61419355\n",
      " 0.6116129  0.60774194 0.61419355 0.61419355 0.6116129  0.60774194\n",
      " 0.64774194 0.65290323 0.64516129 0.63870968 0.63612903 0.64387097\n",
      " 0.63354839 0.63612903 0.62967742 0.63483871 0.6283871  0.62451613\n",
      " 0.63096774 0.63483871 0.63354839 0.63096774 0.63354839 0.64\n",
      " 0.63483871 0.63483871 0.62580645 0.6283871  0.62322581 0.62193548\n",
      " 0.62322581 0.63096774 0.62451613 0.62193548 0.62322581 0.63096774\n",
      " 0.62451613 0.62193548 0.62451613 0.62709677 0.61935484 0.62322581\n",
      " 0.60903226 0.60387097 0.60903226 0.6116129  0.60903226 0.60387097\n",
      " 0.60903226 0.6116129  0.60903226 0.60387097 0.60903226 0.6116129 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Real object model]: 0.6486486486486487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.65677419 0.66967742 0.67612903 0.67354839 0.65548387 0.65419355\n",
      " 0.65677419 0.66064516 0.6516129  0.65548387 0.66064516 0.66451613\n",
      " 0.66967742 0.67096774 0.6683871  0.6683871  0.66322581 0.66709677\n",
      " 0.66451613 0.67225806 0.6516129  0.65419355 0.65935484 0.66193548\n",
      " 0.65419355 0.66064516 0.66322581 0.66322581 0.65419355 0.66064516\n",
      " 0.66322581 0.66322581 0.65935484 0.65935484 0.65806452 0.66451613\n",
      " 0.64516129 0.65032258 0.65032258 0.65032258 0.64516129 0.65032258\n",
      " 0.65032258 0.65032258 0.64516129 0.65032258 0.65032258 0.65032258\n",
      " 0.65677419 0.66451613 0.67096774 0.66967742 0.66193548 0.66709677\n",
      " 0.67483871 0.67225806 0.66193548 0.65677419 0.66193548 0.66451613\n",
      " 0.67096774 0.67096774 0.67483871 0.67741935 0.66580645 0.66709677\n",
      " 0.67870968 0.67741935 0.66064516 0.66193548 0.66580645 0.66580645\n",
      " 0.65806452 0.65935484 0.65806452 0.65935484 0.65806452 0.65935484\n",
      " 0.65806452 0.65935484 0.65806452 0.65806452 0.65806452 0.65935484\n",
      " 0.64774194 0.64645161 0.64645161 0.64774194 0.64774194 0.64645161\n",
      " 0.64645161 0.64774194 0.64774194 0.64645161 0.64645161 0.64774194\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66322581 0.66322581 0.66193548 0.65806452 0.65677419 0.65935484\n",
      " 0.65677419 0.66451613 0.65677419 0.66322581 0.66193548 0.66322581\n",
      " 0.66064516 0.66709677 0.66709677 0.67354839 0.67870968 0.67741935\n",
      " 0.67741935 0.67483871 0.65935484 0.65806452 0.66322581 0.66193548\n",
      " 0.65419355 0.65935484 0.66064516 0.66322581 0.65419355 0.65935484\n",
      " 0.66064516 0.66322581 0.65290323 0.65290323 0.65806452 0.66064516\n",
      " 0.64516129 0.64774194 0.65032258 0.65032258 0.64516129 0.64774194\n",
      " 0.65032258 0.65032258 0.64516129 0.64774194 0.65032258 0.65032258\n",
      " 0.66967742 0.67225806 0.67483871 0.67096774 0.6683871  0.67225806\n",
      " 0.67225806 0.67354839 0.65806452 0.65935484 0.65806452 0.66322581\n",
      " 0.66709677 0.6683871  0.6683871  0.6683871  0.66967742 0.67225806\n",
      " 0.67354839 0.67483871 0.65677419 0.65548387 0.66322581 0.65806452\n",
      " 0.66193548 0.66064516 0.66322581 0.66193548 0.66193548 0.66064516\n",
      " 0.66322581 0.66193548 0.66451613 0.66451613 0.65935484 0.65935484\n",
      " 0.64645161 0.64516129 0.64516129 0.64645161 0.64645161 0.64516129\n",
      " 0.64516129 0.64645161 0.64645161 0.64516129 0.64516129 0.64645161\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.65677419 0.66967742 0.67612903 0.67354839 0.65548387 0.65419355\n",
      " 0.65677419 0.66064516 0.6516129  0.65548387 0.66064516 0.66451613\n",
      " 0.66967742 0.67096774 0.6683871  0.6683871  0.66322581 0.66709677\n",
      " 0.66451613 0.67225806 0.6516129  0.65419355 0.65935484 0.66193548\n",
      " 0.65419355 0.66064516 0.66322581 0.66322581 0.65419355 0.66064516\n",
      " 0.66322581 0.66322581 0.65935484 0.65935484 0.65806452 0.66451613\n",
      " 0.64516129 0.65032258 0.65032258 0.65032258 0.64516129 0.65032258\n",
      " 0.65032258 0.65032258 0.64516129 0.65032258 0.65032258 0.65032258\n",
      " 0.65677419 0.66322581 0.67096774 0.66967742 0.66193548 0.66709677\n",
      " 0.67483871 0.67225806 0.66193548 0.65677419 0.66193548 0.66451613\n",
      " 0.67096774 0.67096774 0.67483871 0.67741935 0.66580645 0.66709677\n",
      " 0.67870968 0.67741935 0.66064516 0.66193548 0.66580645 0.66580645\n",
      " 0.65806452 0.65935484 0.65806452 0.65935484 0.65806452 0.65935484\n",
      " 0.65806452 0.65935484 0.65806452 0.65806452 0.65806452 0.65935484\n",
      " 0.64774194 0.64645161 0.64645161 0.64774194 0.64774194 0.64645161\n",
      " 0.64645161 0.64774194 0.64774194 0.64645161 0.64645161 0.64774194\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.65677419 0.66967742 0.67612903 0.67354839 0.65548387 0.65419355\n",
      " 0.65677419 0.66064516 0.6516129  0.65548387 0.66064516 0.66451613\n",
      " 0.66967742 0.67096774 0.6683871  0.6683871  0.66322581 0.66709677\n",
      " 0.66451613 0.67225806 0.6516129  0.65419355 0.65935484 0.66193548\n",
      " 0.65419355 0.66064516 0.66322581 0.66322581 0.65419355 0.66064516\n",
      " 0.66322581 0.66322581 0.65935484 0.65935484 0.65806452 0.66451613\n",
      " 0.64516129 0.65032258 0.65032258 0.65032258 0.64516129 0.65032258\n",
      " 0.65032258 0.65032258 0.64516129 0.65032258 0.65032258 0.65032258\n",
      " 0.65677419 0.66451613 0.67096774 0.66967742 0.66193548 0.66709677\n",
      " 0.67483871 0.67225806 0.66193548 0.65677419 0.66193548 0.66451613\n",
      " 0.67096774 0.67096774 0.67483871 0.67741935 0.66580645 0.66709677\n",
      " 0.67870968 0.67741935 0.66064516 0.66193548 0.66580645 0.66580645\n",
      " 0.65806452 0.65935484 0.65806452 0.65935484 0.65806452 0.65935484\n",
      " 0.65806452 0.65935484 0.65806452 0.65806452 0.65806452 0.65935484\n",
      " 0.64774194 0.64645161 0.64645161 0.64774194 0.64774194 0.64645161\n",
      " 0.64645161 0.64774194 0.64774194 0.64645161 0.64645161 0.64774194\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.65677419 0.66967742 0.67612903 0.67354839 0.65548387 0.65419355\n",
      " 0.65677419 0.66064516 0.6516129  0.65548387 0.66064516 0.66451613\n",
      " 0.66967742 0.67096774 0.6683871  0.6683871  0.66322581 0.66709677\n",
      " 0.66451613 0.67225806 0.6516129  0.65419355 0.65935484 0.66193548\n",
      " 0.65419355 0.66064516 0.66322581 0.66322581 0.65419355 0.66064516\n",
      " 0.66322581 0.66322581 0.65935484 0.65935484 0.65806452 0.66451613\n",
      " 0.64516129 0.65032258 0.65032258 0.65032258 0.64516129 0.65032258\n",
      " 0.65032258 0.65032258 0.64516129 0.65032258 0.65032258 0.65032258\n",
      " 0.65677419 0.66451613 0.67096774 0.66967742 0.66193548 0.66709677\n",
      " 0.67483871 0.67225806 0.66193548 0.65677419 0.66193548 0.66451613\n",
      " 0.67096774 0.67096774 0.67483871 0.67741935 0.66580645 0.66709677\n",
      " 0.67870968 0.67741935 0.66064516 0.66193548 0.66580645 0.66580645\n",
      " 0.65806452 0.65935484 0.65806452 0.65935484 0.65806452 0.65935484\n",
      " 0.65806452 0.65935484 0.65806452 0.65806452 0.65806452 0.65935484\n",
      " 0.64774194 0.64645161 0.64645161 0.64774194 0.64774194 0.64645161\n",
      " 0.64645161 0.64774194 0.64774194 0.64645161 0.64645161 0.64774194]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Mind Map]: 0.6718146718146718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.60129032 0.60387097 0.6        0.60129032 0.62451613 0.61032258\n",
      " 0.60645161 0.61548387 0.60258065 0.6        0.61032258 0.61290323\n",
      " 0.61419355 0.6        0.59483871 0.60645161 0.59612903 0.6116129\n",
      " 0.61290323 0.61935484 0.60387097 0.60129032 0.62064516 0.61548387\n",
      " 0.59096774 0.59741935 0.59483871 0.60258065 0.59096774 0.59741935\n",
      " 0.59483871 0.60258065 0.60129032 0.60129032 0.61419355 0.61806452\n",
      " 0.61032258 0.60903226 0.61032258 0.61032258 0.61032258 0.60903226\n",
      " 0.61032258 0.61032258 0.61032258 0.60903226 0.61032258 0.61032258\n",
      " 0.61419355 0.62193548 0.62322581 0.62322581 0.60258065 0.61548387\n",
      " 0.61935484 0.61935484 0.61806452 0.62322581 0.6283871  0.62322581\n",
      " 0.59870968 0.61290323 0.62064516 0.61935484 0.61677419 0.62451613\n",
      " 0.63612903 0.62709677 0.61548387 0.61677419 0.61419355 0.61290323\n",
      " 0.58064516 0.59483871 0.61032258 0.6116129  0.58064516 0.59483871\n",
      " 0.61032258 0.6116129  0.60774194 0.61032258 0.60774194 0.60516129\n",
      " 0.60516129 0.60516129 0.60903226 0.60774194 0.60516129 0.60516129\n",
      " 0.60903226 0.60774194 0.60516129 0.60516129 0.60903226 0.60774194\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.60645161 0.60258065 0.61032258 0.61419355 0.60645161 0.6\n",
      " 0.61032258 0.60387097 0.61032258 0.60774194 0.61548387 0.61677419\n",
      " 0.61548387 0.60903226 0.60516129 0.61419355 0.59354839 0.59612903\n",
      " 0.59225806 0.60129032 0.59483871 0.59612903 0.60516129 0.59870968\n",
      " 0.57935484 0.58580645 0.58967742 0.59354839 0.57935484 0.58580645\n",
      " 0.58967742 0.59354839 0.59225806 0.60387097 0.59870968 0.60645161\n",
      " 0.60387097 0.61032258 0.61032258 0.61419355 0.60387097 0.61032258\n",
      " 0.61032258 0.61419355 0.60387097 0.61032258 0.61032258 0.61419355\n",
      " 0.60387097 0.62322581 0.61419355 0.61548387 0.61290323 0.61032258\n",
      " 0.60645161 0.61935484 0.61548387 0.60903226 0.6116129  0.61548387\n",
      " 0.60903226 0.60903226 0.61290323 0.62322581 0.61032258 0.61419355\n",
      " 0.61935484 0.62193548 0.6        0.6116129  0.61419355 0.62322581\n",
      " 0.58967742 0.6        0.60774194 0.6116129  0.58967742 0.6\n",
      " 0.60774194 0.6116129  0.60774194 0.6116129  0.61419355 0.6116129\n",
      " 0.60645161 0.60774194 0.60774194 0.60516129 0.60645161 0.60774194\n",
      " 0.60774194 0.60516129 0.60645161 0.60774194 0.60774194 0.60516129\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.60129032 0.60387097 0.6        0.60129032 0.62451613 0.61032258\n",
      " 0.60645161 0.61548387 0.60258065 0.6        0.61032258 0.61290323\n",
      " 0.61419355 0.6        0.59483871 0.60645161 0.59612903 0.6116129\n",
      " 0.61290323 0.61935484 0.60387097 0.60129032 0.62064516 0.61548387\n",
      " 0.59096774 0.59741935 0.59483871 0.60258065 0.59096774 0.59741935\n",
      " 0.59483871 0.60258065 0.60129032 0.60129032 0.61419355 0.61806452\n",
      " 0.61032258 0.60903226 0.61032258 0.61032258 0.61032258 0.60903226\n",
      " 0.61032258 0.61032258 0.61032258 0.60903226 0.61032258 0.61032258\n",
      " 0.61419355 0.62322581 0.62193548 0.62322581 0.60258065 0.61548387\n",
      " 0.61935484 0.61935484 0.61806452 0.62322581 0.6283871  0.62322581\n",
      " 0.59870968 0.61290323 0.62064516 0.61935484 0.61677419 0.62451613\n",
      " 0.63612903 0.62709677 0.61548387 0.61677419 0.61419355 0.61290323\n",
      " 0.58064516 0.59483871 0.61032258 0.6116129  0.58064516 0.59483871\n",
      " 0.61032258 0.6116129  0.60774194 0.61032258 0.60774194 0.60516129\n",
      " 0.60516129 0.60516129 0.60903226 0.60774194 0.60516129 0.60516129\n",
      " 0.60903226 0.60774194 0.60516129 0.60516129 0.60903226 0.60774194\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.60129032 0.60387097 0.6        0.60129032 0.62451613 0.61032258\n",
      " 0.60645161 0.61548387 0.60258065 0.6        0.61032258 0.61290323\n",
      " 0.61419355 0.6        0.59483871 0.60645161 0.59612903 0.6116129\n",
      " 0.61290323 0.61935484 0.60387097 0.60129032 0.62064516 0.61548387\n",
      " 0.59096774 0.59741935 0.59483871 0.60258065 0.59096774 0.59741935\n",
      " 0.59483871 0.60258065 0.60129032 0.60129032 0.61419355 0.61806452\n",
      " 0.61032258 0.60903226 0.61032258 0.61032258 0.61032258 0.60903226\n",
      " 0.61032258 0.61032258 0.61032258 0.60903226 0.61032258 0.61032258\n",
      " 0.61419355 0.62193548 0.62322581 0.62322581 0.60258065 0.61548387\n",
      " 0.61935484 0.61935484 0.61806452 0.62322581 0.6283871  0.62322581\n",
      " 0.59870968 0.61290323 0.62064516 0.61935484 0.61677419 0.62451613\n",
      " 0.63612903 0.62709677 0.61548387 0.61677419 0.61419355 0.61290323\n",
      " 0.58064516 0.59483871 0.61032258 0.6116129  0.58064516 0.59483871\n",
      " 0.61032258 0.6116129  0.60774194 0.61032258 0.60774194 0.60516129\n",
      " 0.60516129 0.60516129 0.60903226 0.60774194 0.60516129 0.60516129\n",
      " 0.60903226 0.60774194 0.60516129 0.60516129 0.60903226 0.60774194\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.60129032 0.60387097 0.6        0.60129032 0.62451613 0.61032258\n",
      " 0.60645161 0.61548387 0.60258065 0.6        0.61032258 0.61290323\n",
      " 0.61419355 0.6        0.59483871 0.60645161 0.59612903 0.6116129\n",
      " 0.61290323 0.61935484 0.60387097 0.60129032 0.62064516 0.61548387\n",
      " 0.59096774 0.59741935 0.59483871 0.60258065 0.59096774 0.59741935\n",
      " 0.59483871 0.60258065 0.60129032 0.60129032 0.61419355 0.61806452\n",
      " 0.61032258 0.60903226 0.61032258 0.61032258 0.61032258 0.60903226\n",
      " 0.61032258 0.61032258 0.61032258 0.60903226 0.61032258 0.61032258\n",
      " 0.61419355 0.62193548 0.62322581 0.62322581 0.60258065 0.61548387\n",
      " 0.61935484 0.61935484 0.61806452 0.62322581 0.6283871  0.62322581\n",
      " 0.59870968 0.61290323 0.62064516 0.61935484 0.61677419 0.62451613\n",
      " 0.63612903 0.62709677 0.61548387 0.61677419 0.61419355 0.61290323\n",
      " 0.58064516 0.59483871 0.61032258 0.6116129  0.58064516 0.59483871\n",
      " 0.61032258 0.6116129  0.60774194 0.61032258 0.60774194 0.60516129\n",
      " 0.60516129 0.60516129 0.60903226 0.60774194 0.60516129 0.60516129\n",
      " 0.60903226 0.60774194 0.60516129 0.60516129 0.60903226 0.60774194]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Multimedia content]: 0.5907335907335908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56387097 0.55483871 0.56516129 0.5716129  0.57548387 0.58193548\n",
      " 0.58709677 0.58451613 0.57290323 0.56903226 0.57419355 0.5716129\n",
      " 0.55483871 0.56387097 0.57419355 0.56645161 0.56258065 0.56516129\n",
      " 0.57548387 0.5716129  0.54451613 0.56129032 0.55483871 0.55354839\n",
      " 0.56516129 0.57290323 0.57032258 0.56       0.56516129 0.57290323\n",
      " 0.57032258 0.56       0.57419355 0.57032258 0.57935484 0.57032258\n",
      " 0.56645161 0.56774194 0.56774194 0.55870968 0.56645161 0.56774194\n",
      " 0.56774194 0.55870968 0.56645161 0.56774194 0.56774194 0.55870968\n",
      " 0.58580645 0.58322581 0.57935484 0.5716129  0.57548387 0.59225806\n",
      " 0.58709677 0.58580645 0.58709677 0.5883871  0.59096774 0.58709677\n",
      " 0.58193548 0.5716129  0.58709677 0.57548387 0.58193548 0.59225806\n",
      " 0.58709677 0.57677419 0.57935484 0.57935484 0.57290323 0.57290323\n",
      " 0.56645161 0.57032258 0.58322581 0.58580645 0.56645161 0.57032258\n",
      " 0.58322581 0.58580645 0.57677419 0.56903226 0.57419355 0.55870968\n",
      " 0.57032258 0.57677419 0.58064516 0.58064516 0.57032258 0.57677419\n",
      " 0.58064516 0.58064516 0.57032258 0.57677419 0.58064516 0.58064516\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56774194 0.56774194 0.58451613 0.58322581 0.56       0.56774194\n",
      " 0.5716129  0.57548387 0.56903226 0.58064516 0.58064516 0.57548387\n",
      " 0.59096774 0.59354839 0.58193548 0.58451613 0.57548387 0.56903226\n",
      " 0.5716129  0.57290323 0.56774194 0.57548387 0.57032258 0.56903226\n",
      " 0.57677419 0.56774194 0.57419355 0.56       0.57677419 0.56774194\n",
      " 0.57419355 0.56       0.57419355 0.57935484 0.57677419 0.57677419\n",
      " 0.56387097 0.56903226 0.57419355 0.55870968 0.56387097 0.56903226\n",
      " 0.57419355 0.55870968 0.56387097 0.56903226 0.57419355 0.55870968\n",
      " 0.59354839 0.58322581 0.58193548 0.57548387 0.58709677 0.58451613\n",
      " 0.57290323 0.5716129  0.57806452 0.58451613 0.58451613 0.58451613\n",
      " 0.56774194 0.57032258 0.57419355 0.57935484 0.58451613 0.59354839\n",
      " 0.59483871 0.57419355 0.58580645 0.58580645 0.58580645 0.58709677\n",
      " 0.56129032 0.5716129  0.57548387 0.58322581 0.56129032 0.5716129\n",
      " 0.57548387 0.58322581 0.57806452 0.57419355 0.58451613 0.58709677\n",
      " 0.57548387 0.57806452 0.58193548 0.57935484 0.57548387 0.57806452\n",
      " 0.58193548 0.57935484 0.57548387 0.57806452 0.58193548 0.57935484\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56387097 0.55483871 0.56516129 0.5716129  0.57548387 0.58193548\n",
      " 0.58709677 0.58451613 0.57290323 0.56903226 0.57419355 0.5716129\n",
      " 0.55483871 0.56387097 0.57419355 0.56645161 0.56258065 0.56516129\n",
      " 0.57548387 0.5716129  0.54451613 0.56129032 0.55483871 0.55354839\n",
      " 0.56516129 0.57290323 0.57032258 0.56       0.56516129 0.57290323\n",
      " 0.57032258 0.56       0.57419355 0.57032258 0.57935484 0.57032258\n",
      " 0.56645161 0.56774194 0.56774194 0.55870968 0.56645161 0.56774194\n",
      " 0.56774194 0.55870968 0.56645161 0.56774194 0.56774194 0.55870968\n",
      " 0.58580645 0.58322581 0.57935484 0.57290323 0.57548387 0.59225806\n",
      " 0.58709677 0.58580645 0.58709677 0.5883871  0.59096774 0.58709677\n",
      " 0.58193548 0.5716129  0.58709677 0.57548387 0.58193548 0.59225806\n",
      " 0.58709677 0.57677419 0.57935484 0.57935484 0.57290323 0.57290323\n",
      " 0.56645161 0.57032258 0.58322581 0.58580645 0.56645161 0.57032258\n",
      " 0.58322581 0.58580645 0.57677419 0.56903226 0.57419355 0.55870968\n",
      " 0.57032258 0.57677419 0.58064516 0.58064516 0.57032258 0.57677419\n",
      " 0.58064516 0.58064516 0.57032258 0.57677419 0.58064516 0.58064516\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56387097 0.55483871 0.56516129 0.5716129  0.57548387 0.58193548\n",
      " 0.58709677 0.58451613 0.57290323 0.56903226 0.57419355 0.5716129\n",
      " 0.55483871 0.56387097 0.57419355 0.56645161 0.56258065 0.56516129\n",
      " 0.57548387 0.5716129  0.54451613 0.56129032 0.55483871 0.55354839\n",
      " 0.56516129 0.57290323 0.57032258 0.56       0.56516129 0.57290323\n",
      " 0.57032258 0.56       0.57419355 0.57032258 0.57935484 0.57032258\n",
      " 0.56645161 0.56774194 0.56774194 0.55870968 0.56645161 0.56774194\n",
      " 0.56774194 0.55870968 0.56645161 0.56774194 0.56774194 0.55870968\n",
      " 0.58580645 0.58322581 0.57935484 0.5716129  0.57548387 0.59225806\n",
      " 0.58709677 0.58580645 0.58709677 0.5883871  0.59096774 0.58709677\n",
      " 0.58193548 0.5716129  0.58709677 0.57548387 0.58193548 0.59225806\n",
      " 0.58709677 0.57677419 0.57935484 0.57935484 0.57290323 0.57290323\n",
      " 0.56645161 0.57032258 0.58322581 0.58580645 0.56645161 0.57032258\n",
      " 0.58322581 0.58580645 0.57677419 0.56903226 0.57419355 0.55870968\n",
      " 0.57032258 0.57677419 0.58064516 0.58064516 0.57032258 0.57677419\n",
      " 0.58064516 0.58064516 0.57032258 0.57677419 0.58064516 0.58064516\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56387097 0.55483871 0.56516129 0.5716129  0.57548387 0.58193548\n",
      " 0.58709677 0.58451613 0.57290323 0.56903226 0.57419355 0.5716129\n",
      " 0.55483871 0.56387097 0.57419355 0.56645161 0.56258065 0.56516129\n",
      " 0.57548387 0.5716129  0.54451613 0.56129032 0.55483871 0.55354839\n",
      " 0.56516129 0.57290323 0.57032258 0.56       0.56516129 0.57290323\n",
      " 0.57032258 0.56       0.57419355 0.57032258 0.57935484 0.57032258\n",
      " 0.56645161 0.56774194 0.56774194 0.55870968 0.56645161 0.56774194\n",
      " 0.56774194 0.55870968 0.56645161 0.56774194 0.56774194 0.55870968\n",
      " 0.58580645 0.58322581 0.57935484 0.5716129  0.57548387 0.59225806\n",
      " 0.58709677 0.58580645 0.58709677 0.5883871  0.59096774 0.58709677\n",
      " 0.58193548 0.5716129  0.58709677 0.57548387 0.58193548 0.59225806\n",
      " 0.58709677 0.57677419 0.57935484 0.57935484 0.57290323 0.57290323\n",
      " 0.56645161 0.57032258 0.58322581 0.58580645 0.56645161 0.57032258\n",
      " 0.58322581 0.58580645 0.57677419 0.56903226 0.57419355 0.55870968\n",
      " 0.57032258 0.57677419 0.58064516 0.58064516 0.57032258 0.57677419\n",
      " 0.58064516 0.58064516 0.57032258 0.57677419 0.58064516 0.58064516]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Interactive Tool]: 0.6254826254826255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59741935 0.61032258 0.61290323 0.60774194 0.59483871 0.59612903\n",
      " 0.58322581 0.5883871  0.59741935 0.60516129 0.61548387 0.60645161\n",
      " 0.59870968 0.61419355 0.60516129 0.60903226 0.60774194 0.60774194\n",
      " 0.61032258 0.60387097 0.59225806 0.60516129 0.61419355 0.61290323\n",
      " 0.60774194 0.6116129  0.61677419 0.61935484 0.60774194 0.6116129\n",
      " 0.61677419 0.61935484 0.60258065 0.60645161 0.61290323 0.61419355\n",
      " 0.59741935 0.59354839 0.6        0.6        0.59741935 0.59354839\n",
      " 0.6        0.6        0.59741935 0.59354839 0.6        0.6\n",
      " 0.59483871 0.59741935 0.6        0.59870968 0.58580645 0.58967742\n",
      " 0.59096774 0.59483871 0.58580645 0.59483871 0.60645161 0.60645161\n",
      " 0.6116129  0.60516129 0.60258065 0.61806452 0.59225806 0.60903226\n",
      " 0.61419355 0.61677419 0.60774194 0.61290323 0.60387097 0.61935484\n",
      " 0.59354839 0.59870968 0.61290323 0.61806452 0.59354839 0.59870968\n",
      " 0.61290323 0.61806452 0.59096774 0.59612903 0.60129032 0.59870968\n",
      " 0.58322581 0.59354839 0.59612903 0.59354839 0.58322581 0.59354839\n",
      " 0.59612903 0.59354839 0.58322581 0.59354839 0.59612903 0.59354839\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.58451613 0.60387097 0.60387097 0.60129032 0.6        0.60645161\n",
      " 0.60387097 0.60903226 0.60774194 0.60129032 0.61032258 0.61419355\n",
      " 0.60129032 0.60387097 0.60516129 0.60645161 0.60258065 0.6\n",
      " 0.59870968 0.60387097 0.60258065 0.60903226 0.60774194 0.61290323\n",
      " 0.60645161 0.61419355 0.61032258 0.60903226 0.60645161 0.61419355\n",
      " 0.61032258 0.60903226 0.60258065 0.6        0.60516129 0.61032258\n",
      " 0.59870968 0.59225806 0.60387097 0.60258065 0.59870968 0.59225806\n",
      " 0.60387097 0.60258065 0.59870968 0.59225806 0.60387097 0.60258065\n",
      " 0.60258065 0.59741935 0.60387097 0.58580645 0.59612903 0.61677419\n",
      " 0.6116129  0.6116129  0.6        0.60774194 0.60774194 0.60903226\n",
      " 0.6        0.61290323 0.61677419 0.61806452 0.60258065 0.61032258\n",
      " 0.60258065 0.60903226 0.60129032 0.60387097 0.60774194 0.60903226\n",
      " 0.60129032 0.6        0.60903226 0.6116129  0.60129032 0.6\n",
      " 0.60903226 0.6116129  0.59354839 0.60258065 0.60129032 0.59870968\n",
      " 0.58193548 0.59483871 0.59225806 0.59096774 0.58193548 0.59483871\n",
      " 0.59225806 0.59096774 0.58193548 0.59483871 0.59225806 0.59096774\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59741935 0.61032258 0.61290323 0.60774194 0.59483871 0.59612903\n",
      " 0.58322581 0.5883871  0.59741935 0.60516129 0.61548387 0.60645161\n",
      " 0.59870968 0.61419355 0.60516129 0.60903226 0.60774194 0.60774194\n",
      " 0.61032258 0.60387097 0.59225806 0.60516129 0.61419355 0.61290323\n",
      " 0.60774194 0.6116129  0.61677419 0.61935484 0.60774194 0.6116129\n",
      " 0.61677419 0.61935484 0.60258065 0.60645161 0.61290323 0.61419355\n",
      " 0.59741935 0.59354839 0.6        0.6        0.59741935 0.59354839\n",
      " 0.6        0.6        0.59741935 0.59354839 0.6        0.6\n",
      " 0.59483871 0.59483871 0.6        0.6        0.58580645 0.58967742\n",
      " 0.59096774 0.59483871 0.58580645 0.59483871 0.60645161 0.60645161\n",
      " 0.6116129  0.60516129 0.60258065 0.61806452 0.59225806 0.60903226\n",
      " 0.61419355 0.61677419 0.60774194 0.61290323 0.60387097 0.61935484\n",
      " 0.59354839 0.59870968 0.61290323 0.61806452 0.59354839 0.59870968\n",
      " 0.61290323 0.61806452 0.59096774 0.59612903 0.60129032 0.59870968\n",
      " 0.58322581 0.59354839 0.59612903 0.59354839 0.58322581 0.59354839\n",
      " 0.59612903 0.59354839 0.58322581 0.59354839 0.59612903 0.59354839\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59741935 0.61032258 0.61290323 0.60774194 0.59483871 0.59612903\n",
      " 0.58322581 0.5883871  0.59741935 0.60516129 0.61548387 0.60645161\n",
      " 0.59870968 0.61419355 0.60516129 0.60903226 0.60774194 0.60774194\n",
      " 0.61032258 0.60387097 0.59225806 0.60516129 0.61419355 0.61290323\n",
      " 0.60774194 0.6116129  0.61677419 0.61935484 0.60774194 0.6116129\n",
      " 0.61677419 0.61935484 0.60258065 0.60645161 0.61290323 0.61419355\n",
      " 0.59741935 0.59354839 0.6        0.6        0.59741935 0.59354839\n",
      " 0.6        0.6        0.59741935 0.59354839 0.6        0.6\n",
      " 0.59483871 0.59741935 0.6        0.59870968 0.58580645 0.58967742\n",
      " 0.59096774 0.59483871 0.58580645 0.59483871 0.60645161 0.60645161\n",
      " 0.6116129  0.60516129 0.60258065 0.61806452 0.59225806 0.60903226\n",
      " 0.61419355 0.61677419 0.60774194 0.61290323 0.60387097 0.61935484\n",
      " 0.59354839 0.59870968 0.61290323 0.61806452 0.59354839 0.59870968\n",
      " 0.61290323 0.61806452 0.59096774 0.59612903 0.60129032 0.59870968\n",
      " 0.58322581 0.59354839 0.59612903 0.59354839 0.58322581 0.59354839\n",
      " 0.59612903 0.59354839 0.58322581 0.59354839 0.59612903 0.59354839\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59741935 0.61032258 0.61290323 0.60774194 0.59483871 0.59612903\n",
      " 0.58322581 0.5883871  0.59741935 0.60516129 0.61548387 0.60645161\n",
      " 0.59870968 0.61419355 0.60516129 0.60903226 0.60774194 0.60774194\n",
      " 0.61032258 0.60387097 0.59225806 0.60516129 0.61419355 0.61290323\n",
      " 0.60774194 0.6116129  0.61677419 0.61935484 0.60774194 0.6116129\n",
      " 0.61677419 0.61935484 0.60258065 0.60645161 0.61290323 0.61419355\n",
      " 0.59741935 0.59354839 0.6        0.6        0.59741935 0.59354839\n",
      " 0.6        0.6        0.59741935 0.59354839 0.6        0.6\n",
      " 0.59483871 0.59741935 0.6        0.59870968 0.58580645 0.58967742\n",
      " 0.59096774 0.59483871 0.58580645 0.59483871 0.60645161 0.60645161\n",
      " 0.6116129  0.60516129 0.60258065 0.61806452 0.59225806 0.60903226\n",
      " 0.61419355 0.61677419 0.60774194 0.61290323 0.60387097 0.61935484\n",
      " 0.59354839 0.59870968 0.61290323 0.61806452 0.59354839 0.59870968\n",
      " 0.61290323 0.61806452 0.59096774 0.59612903 0.60129032 0.59870968\n",
      " 0.58322581 0.59354839 0.59612903 0.59354839 0.58322581 0.59354839\n",
      " 0.59612903 0.59354839 0.58322581 0.59354839 0.59612903 0.59354839]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Technology-supported learning include computer-based training systems]: 0.5675675675675675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1200 fits failed out of a total of 3600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janice\\anaconda3\\envs\\dspenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62193548 0.61677419 0.62322581 0.63096774 0.6116129  0.62967742\n",
      " 0.63225806 0.62967742 0.61935484 0.62709677 0.62451613 0.62193548\n",
      " 0.63612903 0.63870968 0.63870968 0.63096774 0.63870968 0.63483871\n",
      " 0.63225806 0.63096774 0.62967742 0.61935484 0.62322581 0.62322581\n",
      " 0.62193548 0.6283871  0.63096774 0.62967742 0.62193548 0.6283871\n",
      " 0.63096774 0.62967742 0.63096774 0.6283871  0.62451613 0.62580645\n",
      " 0.61935484 0.62193548 0.6283871  0.62709677 0.61935484 0.62193548\n",
      " 0.6283871  0.62709677 0.61935484 0.62193548 0.6283871  0.62709677\n",
      " 0.63225806 0.63225806 0.63612903 0.63225806 0.61548387 0.62322581\n",
      " 0.6283871  0.63225806 0.62709677 0.64387097 0.64387097 0.64387097\n",
      " 0.63612903 0.6283871  0.63225806 0.63354839 0.62967742 0.63741935\n",
      " 0.64516129 0.64258065 0.61935484 0.62580645 0.6283871  0.63225806\n",
      " 0.63612903 0.63870968 0.63870968 0.64       0.63612903 0.63870968\n",
      " 0.63870968 0.64       0.63096774 0.61935484 0.62322581 0.62709677\n",
      " 0.62451613 0.62451613 0.62451613 0.62193548 0.62451613 0.62451613\n",
      " 0.62451613 0.62193548 0.62451613 0.62451613 0.62451613 0.62193548\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.61419355 0.62064516 0.62709677 0.62064516 0.63354839 0.64645161\n",
      " 0.63741935 0.63354839 0.62580645 0.63870968 0.62451613 0.6283871\n",
      " 0.63096774 0.6283871  0.63870968 0.6283871  0.64       0.64516129\n",
      " 0.64129032 0.63870968 0.63096774 0.63741935 0.63096774 0.62709677\n",
      " 0.62580645 0.6283871  0.62967742 0.63354839 0.62580645 0.6283871\n",
      " 0.62967742 0.63354839 0.62193548 0.63096774 0.62709677 0.62709677\n",
      " 0.62451613 0.62451613 0.62580645 0.62967742 0.62451613 0.62451613\n",
      " 0.62580645 0.62967742 0.62451613 0.62451613 0.62580645 0.62967742\n",
      " 0.62967742 0.62967742 0.6283871  0.63096774 0.63354839 0.63870968\n",
      " 0.63354839 0.63870968 0.63612903 0.63612903 0.64258065 0.64387097\n",
      " 0.6283871  0.63612903 0.63870968 0.64       0.62322581 0.63096774\n",
      " 0.64258065 0.63870968 0.62322581 0.62709677 0.63483871 0.63741935\n",
      " 0.63483871 0.63612903 0.62967742 0.63870968 0.63483871 0.63612903\n",
      " 0.62967742 0.63870968 0.62967742 0.62709677 0.62322581 0.62709677\n",
      " 0.62193548 0.62451613 0.62322581 0.62064516 0.62193548 0.62451613\n",
      " 0.62322581 0.62064516 0.62193548 0.62451613 0.62322581 0.62064516\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62193548 0.61806452 0.62322581 0.63096774 0.6116129  0.62967742\n",
      " 0.63225806 0.62967742 0.61935484 0.62709677 0.62451613 0.62193548\n",
      " 0.63612903 0.63870968 0.63870968 0.63096774 0.63870968 0.63483871\n",
      " 0.63225806 0.63096774 0.62967742 0.61935484 0.62322581 0.62322581\n",
      " 0.62193548 0.6283871  0.63096774 0.62967742 0.62193548 0.6283871\n",
      " 0.63096774 0.62967742 0.63096774 0.6283871  0.62451613 0.62580645\n",
      " 0.61935484 0.62193548 0.6283871  0.62709677 0.61935484 0.62193548\n",
      " 0.6283871  0.62709677 0.61935484 0.62193548 0.6283871  0.62709677\n",
      " 0.63225806 0.63096774 0.63612903 0.63225806 0.61548387 0.62322581\n",
      " 0.6283871  0.63225806 0.62709677 0.64387097 0.64387097 0.64387097\n",
      " 0.63612903 0.6283871  0.63225806 0.63354839 0.62967742 0.63741935\n",
      " 0.64516129 0.64258065 0.61935484 0.62580645 0.6283871  0.63225806\n",
      " 0.63612903 0.63870968 0.63870968 0.64       0.63612903 0.63870968\n",
      " 0.63870968 0.64       0.63096774 0.61935484 0.62322581 0.62709677\n",
      " 0.62451613 0.62451613 0.62451613 0.62193548 0.62451613 0.62451613\n",
      " 0.62451613 0.62193548 0.62451613 0.62451613 0.62451613 0.62193548\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62193548 0.61677419 0.62322581 0.63096774 0.6116129  0.62967742\n",
      " 0.63225806 0.62967742 0.61935484 0.62709677 0.62451613 0.62193548\n",
      " 0.63612903 0.63870968 0.63870968 0.63096774 0.63870968 0.63483871\n",
      " 0.63225806 0.63096774 0.62967742 0.61935484 0.62322581 0.62322581\n",
      " 0.62193548 0.6283871  0.63096774 0.62967742 0.62193548 0.6283871\n",
      " 0.63096774 0.62967742 0.63096774 0.6283871  0.62451613 0.62580645\n",
      " 0.61935484 0.62193548 0.6283871  0.62709677 0.61935484 0.62193548\n",
      " 0.6283871  0.62709677 0.61935484 0.62193548 0.6283871  0.62709677\n",
      " 0.63225806 0.63225806 0.63612903 0.63225806 0.61548387 0.62322581\n",
      " 0.6283871  0.63225806 0.62709677 0.64387097 0.64387097 0.64387097\n",
      " 0.63612903 0.6283871  0.63225806 0.63354839 0.62967742 0.63741935\n",
      " 0.64516129 0.64258065 0.61935484 0.62580645 0.6283871  0.63225806\n",
      " 0.63612903 0.63870968 0.63870968 0.64       0.63612903 0.63870968\n",
      " 0.63870968 0.64       0.63096774 0.61935484 0.62322581 0.62709677\n",
      " 0.62451613 0.62451613 0.62451613 0.62193548 0.62451613 0.62451613\n",
      " 0.62451613 0.62193548 0.62451613 0.62451613 0.62451613 0.62193548\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62193548 0.61677419 0.62322581 0.63096774 0.6116129  0.62967742\n",
      " 0.63225806 0.62967742 0.61935484 0.62709677 0.62451613 0.62193548\n",
      " 0.63612903 0.63870968 0.63870968 0.63096774 0.63870968 0.63483871\n",
      " 0.63225806 0.63096774 0.62967742 0.61935484 0.62322581 0.62322581\n",
      " 0.62193548 0.6283871  0.63096774 0.62967742 0.62193548 0.6283871\n",
      " 0.63096774 0.62967742 0.63096774 0.6283871  0.62451613 0.62580645\n",
      " 0.61935484 0.62193548 0.6283871  0.62709677 0.61935484 0.62193548\n",
      " 0.6283871  0.62709677 0.61935484 0.62193548 0.6283871  0.62709677\n",
      " 0.63225806 0.63225806 0.63612903 0.63225806 0.61548387 0.62322581\n",
      " 0.6283871  0.63225806 0.62709677 0.64387097 0.64387097 0.64387097\n",
      " 0.63612903 0.6283871  0.63225806 0.63354839 0.62967742 0.63741935\n",
      " 0.64516129 0.64258065 0.61935484 0.62580645 0.6283871  0.63225806\n",
      " 0.63612903 0.63870968 0.63870968 0.64       0.63612903 0.63870968\n",
      " 0.63870968 0.64       0.63096774 0.61935484 0.62322581 0.62709677\n",
      " 0.62451613 0.62451613 0.62451613 0.62193548 0.62451613 0.62451613\n",
      " 0.62451613 0.62193548 0.62451613 0.62451613 0.62451613 0.62193548]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects [Intelligent computer-aided instruction systems]: 0.637065637065637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Model/rf_model_3.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty dictionary to hold the classifiers\n",
    "best_estimators = {}\n",
    "\n",
    "# Loop through each learning object\n",
    "for col in target.columns:\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 150, 200, 250],  # Expanded number of trees\n",
    "        'max_depth': [None, 10, 20, 30, 50],  # Expanded maximum depth of trees\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum samples for node splitting\n",
    "        'min_samples_leaf': [1, 2, 4, 8],  # Minimum samples required at leaf nodes\n",
    "        'max_features': ['auto', 'sqrt', 'log2']  # Maximum number of features considered for splitting\n",
    "        # Add other hyperparameters to tune\n",
    "    }\n",
    "    \n",
    "    # Instantiate GridSearchCV for RandomForestClassifier\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Fit the grid search to your data for the current learning object\n",
    "    grid_search.fit(X_train, y_train[col])\n",
    "    \n",
    "    # Get the best parameters and best estimator for the current learning object\n",
    "    best_params = grid_search.best_params_\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    \n",
    "    # Store the best estimator in the classifiers dictionary\n",
    "    best_estimators[col] = best_estimator\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test[col], y_pred)\n",
    "    print(f\"Model Accuracy for {col}: {accuracy}\")\n",
    "\n",
    "    \n",
    "# Save the best estimators using joblib\n",
    "joblib.dump(best_estimators, \"Model/rf_model_3.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d1f062f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "print('best_params:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd17b432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy for Learning Objects:\n",
      "Learning Objects [Slide presentation]: 0.5868725868725869\n",
      "Learning Objects [Book]: 0.722007722007722\n",
      "Learning Objects [Lecture Note]: 0.7027027027027027\n",
      "Learning Objects [Educational game]: 0.6486486486486487\n",
      "Learning Objects [Video]: 0.6061776061776062\n",
      "Learning Objects [Audio-recorded lecture]: 0.7065637065637066\n",
      "Learning Objects [Animated instruction]: 0.5907335907335908\n",
      "Learning Objects [Real object model]: 0.6486486486486487\n",
      "Learning Objects [Mind Map]: 0.6718146718146718\n",
      "Learning Objects [Multimedia content]: 0.5907335907335908\n",
      "Learning Objects [Interactive Tool]: 0.6254826254826255\n",
      "Learning Objects [Technology-supported learning include computer-based training systems]: 0.5675675675675675\n",
      "Learning Objects [Intelligent computer-aided instruction systems]: 0.637065637065637\n",
      "Mean accuracy:  0.6388476388476388\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing set for each learning object\n",
    "y_pred = pd.DataFrame({col: classifier.predict(X_test) for col, classifier in best_estimators.items()})\n",
    "\n",
    "sum_acc = 0\n",
    "mean_acc = 0\n",
    "\n",
    "# Evaluate the model for each learning object\n",
    "accuracy = {col: accuracy_score(y_test[col], y_pred[col]) for col in target.columns}\n",
    "print(\"Model Accuracy for Learning Objects:\")\n",
    "for col, acc in accuracy.items():\n",
    "    print(f\"{col}: {acc}\")\n",
    "    sum_acc += acc\n",
    "    \n",
    "mean_acc = sum_acc/13\n",
    "print(\"Mean accuracy: \", mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb55ac",
   "metadata": {},
   "source": [
    "test_size=0.1 --    \n",
    "test_size=0.2 -- 0.7729 -- 0.6386  0.63647  \n",
    "model 3= 0.6388"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f50231",
   "metadata": {},
   "source": [
    "### Make predictions on a new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7245e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = joblib.load(\"Model/rf_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21883470",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Dataset/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66fe2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Objects [Slide presentation]</th>\n",
       "      <th>Learning Objects [Book]</th>\n",
       "      <th>Learning Objects [Lecture Note]</th>\n",
       "      <th>Learning Objects [Educational game]</th>\n",
       "      <th>Learning Objects [Video]</th>\n",
       "      <th>Learning Objects [Audio-recorded lecture]</th>\n",
       "      <th>Learning Objects [Animated instruction]</th>\n",
       "      <th>Learning Objects [Real object model]</th>\n",
       "      <th>Learning Objects [Mind Map]</th>\n",
       "      <th>Learning Objects [Multimedia content]</th>\n",
       "      <th>Learning Objects [Interactive Tool]</th>\n",
       "      <th>Learning Objects [Technology-supported learning include computer-based training systems]</th>\n",
       "      <th>Learning Objects [Intelligent computer-aided instruction systems]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning Objects [Slide presentation]  Learning Objects [Book]  \\\n",
       "0                                      1                        0   \n",
       "1                                      1                        1   \n",
       "2                                      1                        0   \n",
       "3                                      0                        0   \n",
       "4                                      1                        0   \n",
       "\n",
       "   Learning Objects [Lecture Note]  Learning Objects [Educational game]  \\\n",
       "0                                1                                    0   \n",
       "1                                1                                    1   \n",
       "2                                1                                    0   \n",
       "3                                1                                    0   \n",
       "4                                1                                    0   \n",
       "\n",
       "   Learning Objects [Video]  Learning Objects [Audio-recorded lecture]  \\\n",
       "0                         0                                          1   \n",
       "1                         1                                          0   \n",
       "2                         1                                          1   \n",
       "3                         0                                          1   \n",
       "4                         0                                          0   \n",
       "\n",
       "   Learning Objects [Animated instruction]  \\\n",
       "0                                        1   \n",
       "1                                        0   \n",
       "2                                        1   \n",
       "3                                        1   \n",
       "4                                        0   \n",
       "\n",
       "   Learning Objects [Real object model]  Learning Objects [Mind Map]  \\\n",
       "0                                     1                            1   \n",
       "1                                     0                            1   \n",
       "2                                     1                            1   \n",
       "3                                     1                            0   \n",
       "4                                     1                            0   \n",
       "\n",
       "   Learning Objects [Multimedia content]  Learning Objects [Interactive Tool]  \\\n",
       "0                                      0                                    0   \n",
       "1                                      1                                    1   \n",
       "2                                      1                                    0   \n",
       "3                                      0                                    0   \n",
       "4                                      0                                    1   \n",
       "\n",
       "   Learning Objects [Technology-supported learning include computer-based training systems]  \\\n",
       "0                                                  0                                          \n",
       "1                                                  1                                          \n",
       "2                                                  0                                          \n",
       "3                                                  0                                          \n",
       "4                                                  0                                          \n",
       "\n",
       "   Learning Objects [Intelligent computer-aided instruction systems]  \n",
       "0                                                  0                  \n",
       "1                                                  1                  \n",
       "2                                                  0                  \n",
       "3                                                  1                  \n",
       "4                                                  0                  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame({col: classifier.predict(data) for col, classifier in rf_model.items()})\n",
    "predictions.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
